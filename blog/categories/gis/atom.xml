<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: GIS | geoMusings]]></title>
  <link href="http://blog.geomusings.com/blog/categories/gis/atom.xml" rel="self"/>
  <link href="http://blog.geomusings.com/"/>
  <updated>2014-07-24T09:31:24-04:00</updated>
  <id>http://blog.geomusings.com/</id>
  <author>
    <name><![CDATA[Bill Dollins]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Lock-In]]></title>
    <link href="http://blog.geomusings.com/2014/07/24/lock-in/"/>
    <updated>2014-07-24T09:13:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/24/lock-in</id>
    <content type="html"><![CDATA[<p>I've been a consultant/programmer/integrator/other for over twenty years now. That's not quite long enough to say I've seen it all but long enough to notice a few patterns. Admittedly, I've spent the vast majority of that time working in the defense world so the patterns may be heavily skewed to that but I think not.</p>

<p style="text-align:center;"> <img src="http://upload.wikimedia.org/wikipedia/commons/9/93/US_Navy_080904-N-9079D-037_Operations_Specialist_3rd_Class_Sarah_M._Hernandez_stands_the_global_command_and_control_systems-maritime_watch.jpg" /></p>

<p>I've run across a number of well-entrenched government-developed systems, such as command-and-control systems, with user interfaces and experiences that many professional designers would consider abhorrent. Yet, I have watched smart, motivated men and women in uniform stand in front of working groups and committees dedicated to "improving" systems and workflows and advocate passionately for these seemingly clunky systems.</p>

<p>Why? Because they know how to use these systems inside and out to meet their missions. User experience is ultimately about comfort and confidence. A user that is comfortable with a system will have a great experience with it regardless of its appearance. DOD tackles this reality through training. For all its faults, there is still no organization better at developing procedures and thoroughly training its people in them. It results in a passionate loyalty for the tools that help them do their jobs and places a very high hurdle in front of any tools that seek to replace current ones.</p>

<!--more-->


<p>This experience has given me a different view of the concept of "lock-in." Over my career, I have heard this term used in a pejorative sense, usually proceeded by the word "vendor." Although I have used the term myself, I usually hear it levelled by a vendor's competitors. It is typically meant to refer to practices a vendor uses to establish barriers to exit for its customers, making it harder for them to choose a competing technology. Such practices can include artificial bundling of unrelated tools, license trickery, half-truths in marketing, and many more; all of which do happen.</p>

<p>Lock-in is a real thing. Lock-in can also be a responsible thing. The organizations I have worked with that make the most effective use of their technology choices are the ones that jump in with both feet and never look back. They develop workflows around their systems; they develop customizations and automation tools to streamline repetitive tasks and embed these in their technology platforms; they send their staff to beginning and advanced training from the vendor; and they document their custom tools well and train their staff on them as well. In short, they lock themselves in.</p>

<p>This is the right and responsible thing to do. An organization, once it has selected a technology, has a responsibility to master it and use it as effectively as it can. If you start applying numbers to all the activities listed above, you will quickly see that it is an investment that far outstrips the original investment in the technology itself. In fact, the cost of the technology itself is often seen as marginal to the overall lifecycle cost, which makes arguments about removing licensing costs, for example, less effective than they would appear to be.</p>

<p>This is true regardless of the provenance of the technology. The original technology has to start to become a hindrance before change is seriously considered, which I am seeing in a few cases these days. But, by and large, the very strong pattern I have seen is that the majority of lock-in originates with users. To fail to recognize that and continue to target the vendor is to miss the point and, ultimately, the target.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Esri UC So Far #EsriUC]]></title>
    <link href="http://blog.geomusings.com/2014/07/16/the-esri-uc-so-far/"/>
    <updated>2014-07-16T12:13:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/16/the-esri-uc-so-far</id>
    <content type="html"><![CDATA[<p>So I'm halfway through the largest geospatial event of the year, attending it for the first time in four years, and I haven't blogged yet. As always, it's a busy week. Because this event draws people from all over the country (and world), my dance card fills up pretty quickly. And, by the way, there's a conference going on.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/plenary_stage.jpg" /></p>

<p>This is the first I've ever attended the Esri User Conference as just an attendee. If it were a video game, I'd be playing it on the easy level. I sat through the entire plenary for the first time in years. It was nice table setting for the rest of the week. As the father of a dancer, I have developed an eye for choreography and there is plenty of it up on the plenary stage. If I were to level one piece of constructive criticism toward the UC, it's that I'd let speakers be themselves a little bit more. That said, the content was delivered smoothly, which is really the larger point.</p>

<!--more-->


<p>The plenary features a lot of demos that are rehearsed to within an inch of their lives. Knowing that, I still found ArcGIS Pro to be interesting. The UI is well-designed to get out the user's way and it's native 64-bit architecture finally allows it to take advantage of system resources in a way that ArcMap never could. The UI is more modern, featuring the ribbon toolbar. Esri seems to have learned a lesson from Microsoft by not re-engineering the UI of a familiar product, but releasing a new product that users can transition to. That was smart. In Microsoft Office terms, the UI upates from the Office 97 feel of ArcMap to more of an Office 2012 feel. Tasks in ArcGIS simplify geoprocessing even more but also run the risk of hiding complexity too much. I hope we're not training another generation of button-clickers.</p>

<p>One of the more subtle announcements in the plenary was that ArcGIS Portal will be included with ArcGIS Server at 10.3. I suspect that's the beginning of a gentle nudge of Server into ArcIMS-like oblivion but that's just my speculation.</p>

<p>The newly-released (not beta) ArcGIS Open Data extension (product?) was shown by <a href="http://twitter.com/ajturner">Andrew Turner</a>. It does a very nice job of putting an almost-GeoCommons-easy interface in front of ArcGIS Online to enable organizations to easily share their data. Andrew showed the implementations for the DC government and the State of Maryland. Maryland, in particular, is exceedingly happy with how quickly it has helped them start sharing data. By chance, I had dinner with some of the development team the night before and it's a motivated team with a diverse skill set not rooted in traditional Esri thinking which, in my opinion, bodes well for what they are doing.</p>

<p>Yesterday, I sat through a couple of sessions on the GeoEvent Processor (GEP), which has already been rebranded as the GeoEvent Extension for Server (or something like that). Those who have tracked my blog over the years know that I've done a lot of work with situational awareness systems. Within that field, I've concentrated a lot on feeds (streaming data sources) and track management so it's something of a geeky passion for me. Esri's previous, long-standing, and wholly awful product in this space was Tracking Server. I would gleefully dance around Tracking Server's funeral pyre.</p>

<p>GEP seems to be a better attempt at solving this problem. In its current incarnation, it simply doesn't have the throughput my customers will need (it currently supports about 800 - 1000 messages per second). To the product team's credit, they know this and are focusing on it. At 10.3, GEP will support "clustering" to increase throughtput. Clustering, however, was not explained during the session and could mean anything so I'll wait and see. The management interfaces and APIs behind GEP are top-notch and enable some of the use cases I've supported in the past. Once the throughput challenges are solved, this could be a product to watch.</p>

<p>You may have noticed a few parenthetical comments above when referring to product names. The reason for that is that Esri's product naming has fallen off the cliff to become just confusing, unwieldy, and random. Couple that with the fact that product names change constantly, or are recycled endlessly (Explorer?), and it's a complete mess. It's always been difficult walking new users through the maze of Esri products and the current naming "convention" doesn't help at all. Esri really needs to circle the wagons on this and simplify things.</p>

<p>That sums up my week so far. I'll try to check in again before I go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gearing Up For the Esri UC]]></title>
    <link href="http://blog.geomusings.com/2014/07/09/gearing-up-for-the-esri-uc/"/>
    <updated>2014-07-09T16:05:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/09/gearing-up-for-the-esri-uc</id>
    <content type="html"><![CDATA[<p>With a house move behind us and a lot of unpacking and other tasks ahead, I am nonetheless getting ready to head out to the <a href="http://www.esri.com/events/user-conference">Esri International User Conference</a> next week. This will be my first time attending since 2010 and is the first UC since then that has aligned with my schedule in a way that I can make it. Of course, the price is right this year as well ($0.00).</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/office_view.png" /></p>

<p>The "big" UC has steadily dropped in significance for me over years as it has become much easier to get Esri-related information through various other channels; primarily through social media and local/regional events such as the <a href="http://www.esri.com/events/federal">FedUC</a> and local dev meetups. The last few trips to San Diego left me feeling that the content presented there is getting increasingly superficial compared to the other events. This year, however, I have been in the midst of building a house and moving so I have not had the time to attend the smaller events. As a result the UC makes sense. I am hoping the recent trend reverses itself.</p>

<p>I still do some Esri-based consulting so it's important to stay current however I can. My government customers are starting to at least ask about ArcGIS Online, so I want to finally get my mind around it as best I can. The messaging around that platform has been so muddled that it's still difficult for me to articulate what productivity advantages, if any, it actually offers. My own experimentation with it has left me wanting. The fact that it has been <a href="http://www.executivegov.com/2014/06/agriculture-dept-grants-fisma-certification-to-esri-cloud-mapping-tech/">certified as FISMA compliant</a> will certainly raise its profile with some of my Federal customers, though that's typically only the first step in a very long process. I'm also curious about ArcGIS Pro.</p>

<p>Unlike previous years, I won't be manning a booth (we sold ours a few years ago) or directly representing a customer so I'll actually have the luxury to attend sessions and meet up with people. There are some people who, unfortunately, I really only see face-to-face at such large magnet events so I'm looking forward to catching up with them, as well as meeting new people. I will, however, be popping into the paper session of one of my co-workers. (It would be awesome if the online agenda provided permalinks to individual sessions.)</p>

<p>So, if you're going to be in at the UC next week and would like to meet up, feel free to ping me on Twitter (<a href="https://twitter.com/billdollins">@billdollins</a>), via e-mail (bill [at] geomusings [dot] com), or drop a comment below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where Ya Been?]]></title>
    <link href="http://blog.geomusings.com/2014/06/20/where-ya-been/"/>
    <updated>2014-06-20T10:14:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/06/20/where-ya-been</id>
    <content type="html"><![CDATA[<p>It's been rather quiet on the blog for a while. Sometimes the posts have to take a back seat to work and other things. This time of year tends to be busy anyway due to the end of the school year and its related activities, but this year has also included one move, construction of a house, and preparations for a second (final) move. In December we sold our house, which I had lived in for nearly 40 years, and moved into temporary quarters while the next house was being built. The sale of the old place was a pretty smooth experience as all of us, especially me, were ready for a change.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/sunrise_crop_small.jpg" /></p>

<p>As a result, the experimentation and small projects which have driven the content of this blog since it started simply had to stop for a while. That's not to say that there has been no activity. I have posted over the last few months related to some mapping work and the "software exhaust" that has resulted from it. It's not really been possible, however, to sit down a create a well-structured discussion of those activies in the way that I would prefer, so I simply haven't.</p>

<!--more-->


<p>Because that work involved the use of <a href="https://www.mapbox.com/foundations/an-open-platform/#mbtiles">MBTiles</a>, it got me a little closer to some of the open-source tools produced by <a href="http://mapbox.com">MapBox</a> in a way that I haven't really needed to before. In addition to <a href="https://github.com/mapbox/mbutil">MBUtil</a> and <a href="https://www.mapbox.com/tilemill/">TileMill</a>, I've been able to use some of their <a href="http://leafletjs.com/">Leaflet</a> Javascript plug-ins for use in data visualiztion. My overall observation is that I've been very impressed with the quality of the tools they are putting out in terms of performance, stability, and elegance. It's been a pleasant experience working with their tools and I've also learned a lot by digging into their source.</p>

<p>It reminds me a lot of the experience I had a few years ago working with the GeoIQ tool set. That tool set resonated with me for the same reasons that the MapBox tool set does now; I feel like the authors of the tools think about problems and approach them in a manner similar to the way I do. As a result, I find the tools comfortable and not something I feel like I am fighting with. I'm not certain that this psychological aspect of software is given much attention but software is really a representation of the author's approach to problem-solving. For me, developing proficiency with a piece of software is establishing a connection with its author in a manner similar to that which is established with the author of a good book. It's one of the reasons I consider releasing open-source code to be such a brave thing.</p>

<p>It's my hope that I'll find ways to keep working with the MapBox tool set more, but such considerations are always driven by project availability and customer demand. In the meantime, there's a move to complete, a new house to settle into, and a summer to enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Personal Geospatial Workflows, May 2014 Edition]]></title>
    <link href="http://blog.geomusings.com/2014/05/20/personal-geospatial-workflows/"/>
    <updated>2014-05-20T08:21:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/05/20/personal-geospatial-workflows</id>
    <content type="html"><![CDATA[<p>I have been spending the past few weeks dealing more with data and mapping than I have in quite a while. It's given me a chance to regain my footing with map-making, reconnect with some end-user tools like <a href="http://www.arc2earth.com">Arc2Earth</a>, and build a little more proficiency with things like <a href="http://www.gdal.org/">GDAL</a>, <a href="http://qgis.org">QGIS</a>, and <a href="https://www.mapbox.com/tilemill/">TileMill</a>. Of course, I've been able to sneak in some coding as I've identified gaps in my workflow.</p>

<p>In a nutshell, I am building base maps for use on disconnected mobile devices. There are two styles of base maps; imagery (really more of an imagery/vector hybrid) and a high-contrast map for use on the outdoor devices and sourced only from vector data. In both cases, I am building MBTiles databases to support the requirement for disconnected operations and to provide consistency in data size and performance.</p>

<p>For the imagery base maps, I was faced with following a data request process that may or may not have resulted in getting imagery in a timely fashion. Alternatively, I was presented with the option of using a tiled map service to get the imagery. Given that I was just making basemaps, this would have been acceptable but for the spotty speed and reliability of the network connection. The ideal solution would be to get only the tiles I need, store them locally, create a geo-referenced image from them, and build a virtual raster table (VRT) for each level.</p>

<!--more-->


<p>Downloading the tiles was easy, but for the VRT to work, each tile needed to be geo-referenced. It was fairly easy to modify the venerable <a href="http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/globalmaptiles.py">globalmaptiles.py</a> to include a routine to create world file parameters for a specified tile. With this, I was able to write out an affine transformation world file for each tile I downloaded. I rolled this whole process up into a <a href="https://github.com/geobabbler/tile-grab">Python script that's available here</a>. Please note that my goal was to create a VRT, so the script flattens out the tiling scheme so that all images are under the appropriate "Z" directory. (This particular server was an ArcGIS Server but the script doesn't care as long as you can provide a valide URL template.)</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bat'><span class='line'>python tile_grab.py -b <span class="m">-158</span>;<span class="m">21</span>;<span class="m">-157</span>;<span class="m">22</span> -d E:\tiles\oahu_img\a -i false -z <span class="m">6</span> -u http:<span class="n">//www.someserver.net/arcgis/rest/services/ImagerySvc/MapServer/tile/{z}/{y}/{x}.png</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>With the tiles downloaded and geo-referenced, it was easy to use the gdalbuildvrt utility to generate the VRT, which can be used in QGIS, TileMill, and ArcGIS Desktop as you prefer.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bat'><span class='line'>gdalbuildvrt <span class="m">6</span>.vrt <span class="m">6</span><span class="n">/*.png</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>It seems a little odd to use tiles to make tiles, but I needed to add some additional data and styling to make the maps do what I needed to do. The downloaded tiles were just a stand-in for what would have been a standard raster data source. The range of useful resolution for a set of tiles is pretty narrow so you'll probably need to grab a few levels and, even then, you'll need to be careful how you use them. In most cases, using local raster/imagery is better but using tiles was fine for my use case and helped mitigate a byzantine data acquisition process. Here's how I set the ranges in ArcGIS (left) and TileMill (right):</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/vrt_zoom2.png" /></p>

<p>Using this approach, I was able to successfully generate my maps using either of two technology mixes. I succeeded in using both TileMill and ArcGIS/Arc2Earth to generate my maps. I ended up doing most of the work in Arc2Earth due to the availability of command-line tools that helped me optimize performance.</p>

<p>Before attempting this method, it's important to make sure that you are not violating any terms of service, license agreements, or attribution requirements in doing so. I knew this wasn't an issue in my case, but such questions need to be answered before you start grabbing data.</p>
]]></content>
  </entry>
  
</feed>
