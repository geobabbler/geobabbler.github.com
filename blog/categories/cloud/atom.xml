<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cloud | geoMusings]]></title>
  <link href="http://blog.geomusings.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://blog.geomusings.com/"/>
  <updated>2013-10-25T10:23:45-04:00</updated>
  <id>http://blog.geomusings.com/</id>
  <author>
    <name><![CDATA[Bill Dollins]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Arkansas Evaluating Geospatial Cloud Infrastructures]]></title>
    <link href="http://blog.geomusings.com/2011/04/29/arkansas-evaluating-geospatial-cloud-infrastructures/"/>
    <updated>2011-04-29T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2011/04/29/arkansas-evaluating-geospatial-cloud-infrastructures</id>
    <content type="html"><![CDATA[<p><a href="http://www.zekiah.com">My company</a> has recently begun working on a project for the Arkansas Geographic Information Office (AGIO) to evaluate options for potentially migrating the <a href="http://www.geostor.arkansas.gov/G6/Home.html">GeoStor</a> geospatial infrastructure to a cloud computing environment. If you are unfamiliar with GeoStor, it is the official geospatial platform for the State of Arkansas and is maintained by AGIO. I realize, as does AGIO, that "the cloud" has become a buzzword that has lost any meaningful context. So, for purposes of discussion, we'll go with the NIST draft definition available here: <a href="http://csrc.nist.gov/publications/drafts/800-145/Draft-SP-800-145_cloud-definition.pdf">http://csrc.nist.gov/publications/drafts/800-145/Draft-SP-800-145_cloud-definition.pdf</a>.</p>

<p>For this effort, we are open to considering any service model but we have strict guidance that the only deployment model of interest is the public cloud model.<!--more--></p>

<p>We are currently engaged in developing a detailed characterization of the current state of GeoStor (software, hardware, support, staffing), its user base and the products and services it provides. From there, we will derive the functional capabilities that are employed by GeoStor and would need to be supported in a new architecture.</p>

<p>At that point, we will distribute a survey so that interested vendors may assess the ability of their products to meet those requirements. We have been directed to "cast the net wide" so as not to miss out on potentially compelling solutions. After some discussion, we decided a post such as this may be a good way to kick off the process.</p>

<p><img alt="The floodgates are open" src="http://verizonvoyager.org/wp-content/uploads/2009/07/flood-gates-550x412.jpg" /></p>

<p>So, if you feel you have a product/service/solution that you would like considered and would like to receive this survey, please send the following information to me by e-mail (<a href="mailto:bill@zekiah.com?subject=GeoStor Survey">bill@zekiah.com</a>):</p>

<ul>
    <li>Your company name</li>

    <li>Name of the relevant point of contact within your company</li>

    <li>E-mail address of the point of contact</li>

    <li>Phone number of the point of contact</li>
</ul>


<p>We realize that we may be opening the floodgates with this but we also feel, given the volatile nature of this space, that more information is better than less. That said, here are a few guidelines:</p>

<ul>
    <li>Please do not attempt to contact me, my company or AGIO about this effort other than to send the information requested above. We are quite busy with characterization efforts and cannot take time away from that to answer questions. The nature of the GeoStor requirements will be evident from the survey.</li>
    <li>This post, the survey or any activity related to it should not be construed as a request for proposal or an intent to engage in a procurement or acquisition of any kind. This is strictly a market survey activity.</li>
    <li>My company is acting solely as a consultant and cannot represent AGIO in any official capacity. No communication from me or my company, including this post, should be considered a representation of the intent or positions of AGIO in any way.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clouds]]></title>
    <link href="http://blog.geomusings.com/2010/07/26/clouds/"/>
    <updated>2010-07-26T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2010/07/26/clouds</id>
    <content type="html"><![CDATA[<p>I spent the vast majority of my time at the 2010 ESRI User Conference working the <a href="http://www.zekiah.com">Zekiah</a>/<a href="http://www.arc2earth.com">Arc2Earth </a>booth. That was fun as I got meet/reconnect with a lot of people but I didn't see much of the conference itself. As a result, I haven't really blogged it.</p>

<p><a href="http://www.esri.com">ESRI</a> continued with the "cloud ready" theme that was rolled out at the Federal User Conference but with more details about how they are moving to "the cloud." This generated a lot of buzz amongst many of the attendees from what I could tell. One of the big new features of Arc2Earth v3 (disclaimer: my company is an Arc2Earth reseller) is Cloud Services. As a result, we had a banner in our booth that had the word "cloud" on it, prompting lots of people to stop.<!--more--></p>

<p>Probably the biggest question I heard related to the cloud was "tell me how it works." Generally, people were not asking us about how Arc2Earth uses the cloud (although there was a lot of interest there but that is not the focus of this post) but rather to explain "the cloud" in conceptual terms. What I concluded was that, while ESRI's focus on the cloud raised the consciousness of cloud computing for many attendees, it did not necessarily raise their understanding of it. I don't particularly consider this a failing on the part of ESRI. I think they did a good job of continuing to flesh out their approach, which is what I would expect at their conference.</p>

<p>{% youtube oAB9Y2CVqZU %}</p>

<p>My problem with the term "the cloud" has always been that it conveys the impression of one monolithic entity that's "out there" to be used. In reality, there are multiple "clouds." Each of them is a massive computing infrastructure that is being exposed for users to rent for their computing needs. This is not unlike the more traditional hard infrastructures such as the power grid or telephone networks. One major difference is that they are not boxed into "natural monopolies" like physical networks can be. With internet access, you can freely pick and choose.</p>

<p>The major infrastructures right now are owned by <a href="http://aws.amazon.com/ec2/">Amazon</a>, <a href="http://code.google.com/appengine/">Google</a> and <a href="http://www.microsoft.com/windowsazure/">Microsoft</a>. There are others but these three seem to be generating the most interest at the moment. Each has a different model for how it exposes its infrastructure for use. Layered onto them are companies that have built service offerings on top of these infrastructures, not unlike how there are different kinds of transportation companies (car rental, taxis, courier services, overnight shipping, trucking companies, etc.) that use the transportation infrastructure. In the geospatial market, the ones that I have the most experience with are <a href="http://www.weogeo.com">WeoGeo</a>, <a href="http://www.arc2earth.com">Arc2Earth</a>, <a href="http://www.fortiusone.com">GeoCommons/GeoIQ</a> and <a href="http://www.esri.com">ESRI</a>. Like the cloud providers, each has a different approach for exposing geospatial data and tools via the cloud infrastructure.</p>

<p>The choice of cloud infrastructure and service provider can greatly affect workflow and also have widely varying costs so it's important to really dig in and understand the various permutations available and make the best choice for your organization. If you are considering moving your geospatial operations to "the cloud", it is worth taking time to understand the various cloud infrastructures, how they operate, and what the various service providers offer.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Couple of Updates]]></title>
    <link href="http://blog.geomusings.com/2010/06/15/a-couple-of-updates/"/>
    <updated>2010-06-15T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2010/06/15/a-couple-of-updates</id>
    <content type="html"><![CDATA[<p>Shortly after my <a href="http://geobabble.wordpress.com/2010/06/02/importing-data-from-geocommons-into-arcmap/">previous post</a>, about browsing and downloading data from <a href="http://www.geocommons.com">GeoCommons</a>, hit the wires, I got quite a few back-channel requests for the code. I sent it out via e-mail to a number of people and then posted it via DropBox. I have finally gotten around to posting it up on Google Code, making things much more manageable. It is now available <a href="http://code.google.com/p/geocommonsbrowser/">here</a>.</p>

<p>I have made a few updates since the original post. Some were administrative but were functional. They are:</p>

<ol>
<li>The code was updated to replace SharpZipLib with <a href="http://dotnetzip.codeplex.com">DotNetZip</a> for handling zip files.</li>
<li>The code now attempts to identify the default KML handler on the user's system and pass KML directly to it for previewing.</li>
<li>The user now gets a wait cursor when the tool is processing downloads and such. This should make it a little more usable.</li>
<li>The code headers had been pasted in from <a href="http://sharpmap.codeplex.com">SharpMap</a> and I missed some references to SharpMap in the text. Those have been corrected.</li>
</ol>


<p>Anyway, thanks for all the interest. It sort of caught me off guard but at least the code is more accessible now. I've got a few more updates planned so this should streamline things.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Importing Data From GeoCommons Into ArcMap]]></title>
    <link href="http://blog.geomusings.com/2010/06/02/importing-data-from-geocommons-into-arcmap/"/>
    <updated>2010-06-02T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2010/06/02/importing-data-from-geocommons-into-arcmap</id>
    <content type="html"><![CDATA[<p><strong>UPDATE:</strong> <em>The code for this post is available at the bottom of the page.</em></p>

<p>I have been doing a lot of development with the <a href="http://resources.esri.com/arcgisserver/apis/silverlight/">ESRI Silverlight API</a> recently. One of the requirements of my project is to be able to dynamically add KML data at runtime. The incorporation of <a href="http://code.google.com/apis/kml/documentation/">KML</a> was handled for us through <a href="http://resources.esri.com/arcgisserver/apis/silverlight/index.cfm?fa=codeGalleryDetails&amp;scriptID=16487">one of the ESRI samples</a> on the resource center so we pretty much just had to integrate that code and test against our use cases. For testing, I typically reached out to <a href="http://www.geocommons.com">GeoCommons</a> since any data set available there can be streamed as KML.</p>

<p>Obviously, this is not my first exposure to GeoCommons but, when discussing it, I found that many of the analysts I spoke with were not aware of it and did not use it much. So I decided to tackle developing a simple ArcMap extension to allow a user to search GeoCommons and then download/add data to ArcMap without the need to manually download, unzip and add the data themselves.<!--more--></p>

<p>GeoCommons, and the GeoIQ platform on which it is based, are produced by <a href="http://www.fortiusone.com/">FortiusOne</a> and are comprised, broadly speaking, of two main components: Finder and Maker. Finder allows you to search/browse data on GeoCommons while Maker allows you to visualize/map the data online and produce map products with nothing but a browser required. That's a gross oversimplification of what GeoCommons can do so I recommend that you kick the tires yourself if you haven't already done so.</p>

<p>Finder supports <a href="http://www.opensearch.org/Home">OpenSearch</a> (see more about this <a href="http://blog.fortiusone.com/2010/01/05/better-know-a-geocommons-feature-opensearch/">here</a>) so the basic search syntax is simple and RESTful. Results can be returned in a number of formats such as JSON, KML and Atom. For example, a query using the search term "oil" and returning results as Atom would simply be:</p>

<p><a href="http://finder.geocommons.com/search.atom?query=oil">http://finder.geocommons.com/search.atom?query=oil</a></p>

<p>We can further refine my query by added parameters such as "limit" to restrict the number of results returned (this can be a very good thing as there's quite a bit of data on GeoCommons) and "bbox", which can used to restrict your search to a specific geographic area. Once you have the search results, you can easily get at the actual data in a number of formats, KML and shapefile (zipped) for example. You specify the format in a RESTful manner by simply changing the URI. The following links point to Maryland zip code boundaries in KML and shapefile formats, respectively:</p>

<p><a href="http://finder.geocommons.com/overlays/22026.kml">http://finder.geocommons.com/overlays/22026.kml</a>
<a href="http://finder.geocommons.com/overlays/22026.zip">http://finder.geocommons.com/overlays/22026.zip</a></p>

<p>So what all of this boils down to is that Finder has a simple search syntax, returns results in a well-known format, and delivers data sets in standard formats. Integrating this into ArcMap should be fairly easy. I started by designing a simple search dialog:</p>

<p><img alt="" class="alignnone size-full wp-image-932" height="374" src="http://geobabble.files.wordpress.com/2010/06/geocommons_form.png" title="A simple GeoCommons query dialog" width="419" /></p>

<p>This post is not intended to be a lesson on how to extend ArcMap so I'll describe the basic components at a high level. I used C# to develop the project and the dialog is built using regular Windows Forms rather than WPF. It uses version 3.5 of the .Net framework. There is a simple toolbar button that creates an instance of the form and passes in a reference to the ArcMap application so that the form can get access to information such as the current extent as needed.</p>

<p>The workflow is fairly simple. Enter a "search term" in the text box (exactly as you would in the Finder web interface), select a "limit" (default is 20) and then click "execute" to run your search. The following is the code behind the "execute" button:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>        IEnvelope bounds = this._mxd.ActiveView.Extent; //current map extent
        bounds.Project(this.getWGS84()); //convert to WGS84 for use in query
        ExecuteSearch(this.txtKeyword.Text, Convert.ToInt32(this.comboBox1.SelectedItem.ToString()), bounds.YMax, bounds.YMin, bounds.XMax, bounds.XMin);
</code></pre>

<p>{% endcodeblock %}</p>

<p>And the ExecuteSearch method that does the work:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>        WebClient request = new WebClient();
        string url = String.Format("http://finder.geocommons.com/search.atom?query={0}&amp;amp;limit={1}&amp;amp;bbox={2},{3},{4},{5}", term, limit.ToString(), west, south, east, north); //format the URI
        request.DownloadStringCompleted += new DownloadStringCompletedEventHandler(request_DownloadStringCompleted); //attach handler for async call
        request.DownloadStringAsync(new Uri(url)); 
</code></pre>

<p>{% endcodeblock %}</p>

<p>These two pieces of code format the URI and make the call to the GeoCommons API. As you can see, it automatically uses the current map extent to bound the query. GeoCommons expects the bounding box to be in WGS84 so I convert the extent before building the URI. The "getWGS84" method is an ArcObjects helper function that I wrote years ago and reuse extensively.</p>

<p>The next part of the workflow is to parse the search results and display them in the "results" list box. To support that, I created the following (very simple) class:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>public class OverlayInfo
{
    public string title { get; set; }
    public string shapelink { get; set; }
    public string kmllink { get; set; }
    public string infolink { get; set; }
}
</code></pre>

<p>{% endcodeblock %}</p>

<p>Basically, I'll create an instance of OverlayInfo for each search result and add it to the results list, using the "title" property as the display member. I could have set up a data contract to bind these directly to the atom entries (and I may still do that) but, for the limited information I am handling at the moment, it was simpler to parse the atom using LINQ to XML. That work is done in the DownloadStringCompleted event handler here:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>        if (e.Error == null)
        {
            this.lstResults.Items.Clear();
            string s = e.Result;
            XElement root = XElement.Parse(s);
            string n = root.Name.LocalName;
            XNamespace atom = "http://www.w3.org/2005/Atom";
            IEnumerable&amp;lt;XElement&amp;gt; entries = root.Elements(atom + "entry"); //get the entry elements
            foreach (XElement entry in entries)
            {
                XElement title = entry.Elements(atom + "title").First(); //query the entry title
                IEnumerable&amp;lt;XElement&amp;gt; links =
                (from el in entry.Elements(atom + "link")
                 where (string)el.Attribute("type") == "application/vnd.google-earth.kml+xml"
                 select el).Take(1); //query the link to the KML resource

                XElement link = null;
                if (links.Count() &amp;gt; 0)
                {
                    link = links.First();
                }
                if (link != null)
                {
                    string kml = links.Attributes("href").First().Value;
                    string shp = kml.Replace("kml", "zip"); //coerce to zip (shapefile) link
                    string info = kml.Replace(".kml", ""); //coerce to general information link
                    OverlayInfo overlay = new OverlayInfo { title = title.Value, shapelink = shp, kmllink = kml, infolink = info }; //create instance of OverlayInfo
                    this.lstResults.Items.Add(overlay);
                }
            }
        }
        else
        {
            MessageBox.Show(e.Error.ToString());
        }
</code></pre>

<p>{% endcodeblock %}</p>

<p>I use LINQ to query the entries out of the feed, iterate them, create an instance of OverlayInfo for each entry and add it to the list box. LINQ is nice for data structures that are known at runtime and I like it better than XPath for querying XML documents (although it's touchier about namespaces).</p>

<p>With that, the user now sees a list of search results. It's time to do something with them. For this pass, they can preview the KML for a selected result or they can download the shapefile data for it. The KML preview simply provides a means to look at the data before getting it.</p>

<p>You will recall that we attached the links to the KML and shapefile data to the OverlayInfo object for each result when we added it to the list so working with it is as simple as this:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>        OverlayInfo overlay = this.lstResults.SelectedItem as OverlayInfo;
        System.Diagnostics.Process.Start(overlay.kmllink);
</code></pre>

<p>{% endcodeblock %}</p>

<p>For this pass, the code simply does a shell execute using the KML link so what this actually does is cause the link to be opened in the default browser which should then prompt you to open the data in Google Earth (or whatever your default KML handler is). In the near future, I will update this to go to GE directly but this worked for now.</p>

<p>The "download" button actually downloads the data and adds it to ArcMap automatically. Handling this is a little more complicated because GeoCommons delivers the shapefile data in a zip file so it needs to be uncompressed first. For handling zip files, I used the open-source <a href="http://dotnetzip.codeplex.com/">DotNetZip</a> to handle the zip files. Here's the bulk of the heavy lifting for handling the shapefiles:</p>

<p>{% codeblock lang:csharp %}</p>

<pre><code>    private void download_Click(object sender, EventArgs e)
    {
        //TODO: move all of this out of the event handler
        string tmp = System.IO.Path.GetTempPath(); //find user's temp folder
        OverlayInfo overlay = this.lstResults.SelectedItem as OverlayInfo;
        string file = System.IO.Path.GetFileName(overlay.shapelink);
        //build all of the necessary file info
        string filebase = file.ToLower().Replace(".zip", "");
        string outfolder = tmp + filebase; //output location
        System.IO.Directory.CreateDirectory(outfolder);
        string outpath = tmp + filebase + "\\" + file;
        //download the data
        WebRequest req = WebRequest.Create(overlay.shapelink);
        WebResponse resp = req.GetResponse();
        Stream strm = resp.GetResponseStream();
        SaveStreamToFile(outpath, strm);
        //unzip the data, getting the name of the .shp file
        string shapefile = Utils.ExtractFiles(outpath, outfolder, false); 
        resp.Close();
        //there may have been an error or no actual shapefile
        if (shapefile != "")
        {
            string workspace = System.IO.Path.GetDirectoryName(shapefile);
            string dataset = System.IO.Path.GetFileName(shapefile);
            dataset = dataset.ToLower().Replace(".shp", "");
            AddShapefile(workspace, dataset, overlay.title);
        }

    }

    private void SaveStreamToFile(string fileFullPath, Stream stream)
    {
        try
        {
            using(Stream fs = File.Open(fileFullPath, FileMode.Create) )
            {
                byte []buf = new byte[1000];
                int iRead = 0;
                do
                {
                    iRead = stream.Read(buf, 0, buf.Length);
                    if (iRead &amp;gt; 0)
                        fs.Write(buf, 0, iRead);
                } while (iRead &amp;gt; 0);
            }                
        }
        catch (Exception ex)
        {
            //TODO: examine and throw
            MessageBox.Show(ex.ToString());
        }
    }

    private void AddShapefile(string folder, string dataset, string description)
    {
        try
        {
            IWorkspaceFactory wkspcfact = new ShapefileWorkspaceFactoryClass() as IWorkspaceFactory;
            //System.IO.Path.
            IFeatureWorkspace wkspc = wkspcfact.OpenFromFile(folder, 0) as IFeatureWorkspace;
            IFeatureLayer lyr = new FeatureLayerClass() as IFeatureLayer;
            IFeatureClass fc = wkspc.OpenFeatureClass(dataset);
            IGeoDataset gds = fc as IGeoDataset;
            IGeoDatasetSchemaEdit edit = gds as IGeoDatasetSchemaEdit;
            if (edit.CanAlterSpatialReference)
            {
                //GeoCommons does not deliver a .prj file with
                //shapefiles. This sets the spatial reference
                edit.AlterSpatialReference(this.getWGS84());
            }
            lyr.FeatureClass = fc;
            lyr.Name = description;
            lyr.SpatialReference = this.getWGS84();
            _mxd.FocusMap.AddLayer(lyr); //_mxd is the current document open in ArcMap
        }
        catch (Exception ex)
        {
            //TODO: put in better user notification
            MessageBox.Show(ex.ToString());
        }
    }
</code></pre>

<p>{% endcodeblock %}</p>

<p>Below is a screen capture of all of this in action (click to enlarge). I zoomed into the Gulf of Mexico region and searched on the term "oil." As can be seen I selected the "Projected Oil Trajectory - Forecast 052610" data set and added it to ArcMap. I manually applied the color ramp after the download.</p>

<p><a href="http://geobabble.files.wordpress.com/2010/06/geocommons_example.png"><img alt="" class="alignnone size-medium wp-image-934" height="178" src="http://geobabble.files.wordpress.com/2010/06/geocommons_example.png?w=300" title="GeoCommons/ArcMap example" width="300" /></a></p>

<p>I was able to turn this around pretty quickly (less than a day) due to two main facts: 1) GeoCommons is well-designed and makes strong use of web standards and open interfaces to expose data in very well-known formats and 2) ArcGIS is, as it has always been, a very extensible platform. If it doesn't support a data source that you want to use, you can probably make it do so (although not always as easily as this example). <del datetime="2010-06-02T22:05:16+00:00">I plan on tidying up the code a bit more and making it available here so keep checking back.</del></p>

<p><strong>UPDATE:</strong> The code for this post is available <a href="http://code.google.com/p/geocommonsbrowser/">here</a>. It includes the source code and the Visual Studio 2008 solution as well as the binary for those of you who are not programmers. It has only been run/tested against ArcGIS Desktop 9.3.1 SP1.</p>
]]></content>
  </entry>
  
</feed>
