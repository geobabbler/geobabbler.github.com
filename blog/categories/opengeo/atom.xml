<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: OpenGeo | geoMusings]]></title>
  <link href="http://blog.geomusings.com/blog/categories/opengeo/atom.xml" rel="self"/>
  <link href="http://blog.geomusings.com/"/>
  <updated>2014-10-10T07:59:47-04:00</updated>
  <id>http://blog.geomusings.com/</id>
  <author>
    <name><![CDATA[Bill Dollins]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DevOps for Geospatial Data]]></title>
    <link href="http://blog.geomusings.com/2013/07/28/devops-for-geospatial-data/"/>
    <updated>2013-07-28T10:54:00-04:00</updated>
    <id>http://blog.geomusings.com/2013/07/28/devops-for-geospatial-data</id>
    <content type="html"><![CDATA[<p>There has been a bit of buzz the past couple of weeks over the <a href="https://github.com/blog/1541-geojson-rendering-improvements">ability of GitHub to render GeoJSON and TopoJSON files</a> automatically using  and embedded <a href="http://leafletjs.com/">Leaflet</a> map and <a href="http://www.mapbox.com/">MapBox</a> technology. This buzz is quite justified as it presents an easy way to simply publish and visualize vector data sets. In the weeks since the initial announcement, the community has begun exploring the limits of GitHub's capability. Probably the two biggest limiting factors are individual file size limits and API rate limits. Some, including myself, are exploring strategies for maximizing the ability to store, disseminate, and visualize data within these confines. For the near term, <a href="https://github.com/">GitHub</a> will probably not be the place to store terabytes of data or act as the CDN for a high-volume mapping application. That is perfectly fine and there is still a great deal of value to be found within GitHub's current generous constraints.</p>

<p style="text-align:center;"><img src="http://blog.geomusings.com/images/posts/geodata-git.png" /></p>

<p>One aspect of GitHub (really, its underlying <a href="http://git-scm.com/">git</a> engine) that is of great interest to me is the ability to perform version control and configuration management on data itself. With GitHub, that currently takes the form of text-based formats such as JSON but it's a start. In my experience supporting various customers over the years, configuration management of data has been a common gap in information operations. The most common, and inadequate, approach to this problem has been through the use of metadata. Almost two decades of viewing out-of-date, incomplete, and inaccurate metadata has given the lie to this approach. Metadata represents a separate maintenance workflow for which many organizations simply do not dedicate resources. Data-set-level metadata is also inadequate for volatile data sets in which individual records are updated frequently.</p>

<!--more-->


<p>I have worked with many organizations that had excellent DevOps processes for managing and deploying application code that simply had no corresponding processes for the data that the code was utilizing. We are long past time for addressing the importance of configuration management for data itself.</p>

<p>That is not to say there have not been approaches to addressing this issue. Since version 8.0, <a href="http://www.esri.com">Esri</a> has had a means of <a href="http://www.esri.com/software/arcgis/geodatabase/multi-user-functionality">versioning enterprise geodatabases</a> that are stored in an RDBMS. This approach does have the ability to track feature-level changes and manages their inclusion in the master version of the data set. Quite frankly, I've never loved the Esri approach to versioning. I think it gives too much responsibility to middleware when it should be the database's sole responsibility to keep itself intact. Also, I have always felt the workflow is a bit too proscribed and takes too much business process decision ability from the data owner. That said, it has been the only real game in town for a long time so I have implemented it many times.</p>

<p><a href="http://www.openstreetmap.org/">OpenStreetMap</a> (OSM) has also been a success story for tracking feature-level version history. Its approach has successfully managed millions of edits to a worldwide database so its utility is certainly proven. Organizations that need to maintain their own data behind their firewalls cannot really make use of OSM itself but the OSM approach is solid.</p>

<p><a href="http://www.zekiah.com">At my own company</a>, we have been working with customers to implement <a href="http://blog.geomusings.com/2012/03/27/configuration-management-for-geospatial-data-models/">configuration management of data models</a> (logical and physical) but we are not really addressing CM of the data managed by those models. Even so, working at the model/schema level has still helped our customers improve their data management workflows by being able to identify versions of data models supported by deployed applications and helping to migrate between versions as needed.</p>

<p>The recent move by GitHub to support visualization of spatial data files actually introduces no new capability in terms of configuration management of data files. Users have always had the ability to store and manage JSON, text, XML and other formats in GitHub and git. My hope is that the visualization capability, and the inevitable exploration it will generate, will shine more light on the issue of data configuration management. In my opinion, this is the most powerful aspect the platform brings.</p>

<p>I'm also encouraged by <a href="http://opengeo.org/">OpenGeo's</a> <a href="https://github.com/opengeo/GeoGit">GeoGit</a> initiative. I have not personally experimented with it yet so I will not speculate on the specifics of its implementation, but I am happy to see OpenGeo recognizing the need for a more open approach to feature-level version control. Since, however, it will be open-source, my hope is that proprietary GIS vendors, or their supporting integrators, will eventually support it as well.</p>

<p>Ultimately, I am happy to see both GitHub and OpenGeo addressing this issue. Although their approaches are different, they offer, in addition to the Esri approach, choices for organizations in terms of workflow. Many data managers, whether for legal, strategic, or other reasons, recognize the importance of maintaining version history of geospatial data records. It is important for everything from parcel mapping to critical infrastructure protection and many other use cases. Historically, there have been very few tools available to address this problem effectively but I am hoping that is starting to change.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FOSS4G North America]]></title>
    <link href="http://blog.geomusings.com/2012/04/12/foss4g-north-america/"/>
    <updated>2012-04-12T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2012/04/12/foss4g-north-america</id>
    <content type="html"><![CDATA[<p>It's rather fitting that the second plenary talk on Wednesday had to do with "firehose" applications since the <a href="http://foss4g-na.org/" target="_blank">FOSS4G North America</a> (FOSS4GNA) conference was something of a firehose in itself. Despite the fact that the event was smaller than the worldwide event in Denver back in October, I came away with the same "full brain" feeling.</p>

<p><img alt="It feels like I never leave this place." src="http://www.visitingdc.com/images/washington-dc-convention-center.jpg" /></p>

<p>Of course, given the recent production release of <a href="http://blog.opengeo.org/2012/04/03/postgis-2-0-released/" target="_blank">PostGIS 2.0</a>, that was kind of the big story for this event. I attended a number of <a href="http://postgis.org/" target="_blank">PostGIS</a>/<a href="http://www.postgresql.org/" target="_blank">PostgreSQL</a>-related sessions and came away with lots of new information. I especially enjoyed <a href="http://twitter.com/pwramsey" target="_blank">Paul Ramsey's</a> "what's new" talk on Wednesday. One thing I enjoy about his talks (here and in Denver) is that he's not afraid to throw sample SQL up on the screen. It's one thing to hear about a new feature but it's another thing entirely to see a concrete example. Some may find the idea of raw SQL in a presentation abhorrent but it worked for me. <!--more--></p>

<p>I've already started playing with PostGIS 2.0 but haven't gotten terribly far so these sessions helped a lot. It's already been pretty well vetted by the community during it's various pre-release stages so it seems remarkably stable. With the addition of topology and raster analysis in addition to 3D/4D indexing and new vector functions, PostGIS probably handles the majority of what most users would need for GIS analysis at the database level. I'm excited to dig deeper.</p>

<p>I also attended a few web-mapping themed talks. If there is one word that consistently and repeatedly passed the lips of most of the presenters, it was "<a href="http://geojson.org/" target="_blank">GeoJSON</a>." I've <a href="http://blog.geomusings.com/?s=geojson" target="_blank">blogged about GeoJSON</a> in the past but it really is the oil that makes the modern geo-web run smoothly. The rate of adoption it is receiving rivals only one other <em>de facto</em> standard that I can recall: you may remember something called the shapefile. The fact is that anyone who's been hunting through their file system looking for the "next shapefile" has been looking in the wrong place. It lives behind that nice map in your browser. The shapefile took off because it solved a problem for what was the frontier of GIS in the 1990s; the transition from heavy workstations to desktop mapping. GeoJSON solves the problem of delivering vector spatial data for today's frontier; the transition from the desktop GIS to web mapping. It's that simple. We don't need another file format to solve an old problem that already has a solution.</p>

<p>Of course, there was a lot more. I got introduced to <a href="http://cartodb.com/" target="_blank">CartoDB</a> in Denver, have <a href="http://blog.geomusings.com/2011/10/13/cartodb-leaflet-easy/" target="_blank">experimented with it</a> since then and was able to get an update on its 1.0 release at one of the sessions. It remains a compelling option for spatial data hosting and <a href="http://vizzuality.com/" target="_blank">Vizzuality</a> are actively engaging with the open-source communities that build the tools they use. On the cartography side of things, <a href="http://stamen.com/" target="_blank">Stamen</a> continues to show how beautiful maps can be made with open-source tools. Similarly, <a href="http://mapbox.com/" target="_blank">MapBox</a> showed how they efficiently deliver high-quality maps on the web. For my money, the unsung hero of what MapBox is doing is <a href="http://mapbox.com/mbtiles-spec/utfgrid/" target="_blank">UTFGrid</a>. I hadn't delved into it deeply until I saw the announcement that <a href="http://www.openlayers.org/" target="_blank">OpenLayers</a> now supports it. UTFGrid is breathtakingly elegant in its approach to delivering richer content with tiles. I expect that, in a few years, UTFGrid will be a standard part under the hood of any web mapping experience, like tiles and slippy maps are today.</p>

<p>The only consistent criticism I heard was that most of the sessions were "too technical." I can see some validity to this, given that the event was held in Washington, DC and many higher-level government or policy types who may have been seeking to conceptually understand open-source geospatial may have been put off. Commercial vendors have addressed this problem with the establishment of "executive tracks" at their events. That said, if that's the biggest criticism that can be leveled, then FOSS4GNA was a rousing success in my book.</p>

<p>I have resolved that I will take at least one of my kids, if not both, to the next FOSS4G event that I attend. The attitude of the open-source crowd is one that I want to rub off on them. Aside from the energy that is exuded from a gathering such as this, the open-source world is full of people who see a problem and just set out to solve it. I want my kids to be exposed to that kind of initiative and understand the power that can come from harnessing it, even if they never go into a technical field.</p>

<p>My understanding is that FOSS4GNA came together very quickly. Given that, it was a very high-quality conference and the organizers should be commended. Early tongue-in-cheek commentary referred to it as the "<a href="http://opengeo.org/" target="_blank">OpenGeo </a>User Conference." I felt like there was good balance and, when I attended sessions given by OpenGeo staff, I didn't feel like I was beaten over the head with the OpenGeo brand. All-in-all, I found my day-and-a-half at FOSS4GNA to be very beneficial and I'm hoping it becomes an annual event. (<strong>Update</strong>: It looks like next year's FOSS4GNA <a href="https://twitter.com/#!/foss4gna/status/190517270928293888" target="_blank">will be in Minnesota</a>.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Taking A Look At PgMap]]></title>
    <link href="http://blog.geomusings.com/2011/08/09/taking-a-look-at-pgmap/"/>
    <updated>2011-08-09T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2011/08/09/taking-a-look-at-pgmap</id>
    <content type="html"><![CDATA[<p>When I blogged about <a href="http://blog.geomusings.com/2011/08/03/ziggis-the-end-of-the-road/">the official end of zigGIS</a> last week, I included a mention of <a href="http://st-links.com/default.aspx">PgMap</a>, a free extension to <a href="http://www.esri.com">ArcMap</a> for direct read/edit of <a href="http://postgis.refractions.net">PostGIS</a> data. Judging from outbound links, there seems to be a good bit of interest in it so I decided to take a look.</p>

<p>To recap, Abe decided to pull the plug on zigGIS due to the fact that <a href="http://events.esri.com/uc/QandA/index.cfm?fuseaction=Answer&amp;ConferenceID=DD02CFE7-1422-2418-7F271831F47A7A31&amp;QuestionID=3949">ESRI will support direct read/edit of spatial databases</a> (as simple features) in 10.1. In my opinion, this is a good development. With native support coming, there was no need to continue with zigGIS. That support, however, will only exist in ArcGIS 10.1. Users of older versions will need to find alternatives. Based on our experience with zigGIS (as well as download data I've seen for the <a href="http://www.zekiah.com/index.php?q=weogeo">WeoGeo toolbar</a>), there are a lot of people (especially outside the US) still using ArcGIS 9.x so demand for an alternative will probably be high for some time.   <!--more--></p>

<p>A quick glance shows me that there are a lot of similarities between PgMap and zigGIS. This is not surprising as there are really only so many ways to approach the task of connecting directly to PostGIS from ArcMap. That said, there are some key differences that are very attractive. They are:</p>

<ol>
    <li>Support for ArcGIS 10</li>
    <li>Support for PostgreSQL 9.x</li>
    <li>Support for PostGIS 1.5</li>
    <li>Availability of a companion product, QMap, for connecting to SQL Server 2008</li>
</ol>


<p>These differences represent capabilities that were frequently requested by zigGIS users but that we simply hadn't gotten to so it's good to see that PgMap has been able to support them. While PgMap is free of charge, it does require a license key. Without the license key, the documentation says that you will only have access to the first 1000 rows of your data. I was only ever able to access the first 100 rows so I suspect there may be a typo in either the docs or the code.</p>

<p>Using ArcGIS 10.0 (ArcEditor), I gave it a very quick run-through. The capability I found is impressive. PgMap allows you to manage connections to PostGIS databases, add PostGIS tables as feature classes and set definition queries for those layers. It appears that PgMap reads the geometry_columns table to list layers. I will need to investigate further to see if it supports views or spatial tables that have not been registered in the geometry_columns table.</p>

<p>There is one key difference between zigGIS and PgMap that I did not mention above and which is quite powerful. PgMap supports the export of ArcGIS feature classes directly to PostGIS (and vice versa). It also appears to support appending to existing PostGIS tables but I have not tested that yet. I used PgMap to export a feature class from a file geodatabase to my PostGIS database and it went flawlessly.</p>

<div style="text-align:center;"><img alt="" height="507" src="http://geobabble.files.wordpress.com/2011/08/pgmap_export.png" title="Using PgMap to export data to PostGIS" width="531" /><div style="text-align:center;font-size: 14px;">PgMap export dialog<br/><br/></div></div>


<p>The resulting PostGIS table was displayed perfectly in QGIS, with all of the attributes coming over well. I have not delved in yet to to see what assumptions PgMap makes in terms of data type conversions between the geodatabase and PostgreSQL.</p>

<div style="text-align:center;"><img alt="" height="315" src="http://geobabble.files.wordpress.com/2011/08/pgmap_qgis.png" title="QGIS displaying PostGIS data imported with PgMap" width="590" /><div style="text-align:center;font-size: 14px;">O Canada!<br/><br/></div></div>


<p>PgMap also integrates seamlessly with the ArcMap editor toolbar for editing. During my investigations, I noticed that layers are loaded into in-memory workspaces so they are natively supported for editing. Edits are then posted back to PostGIS when saved. This is similar to the approach that zigGIS used and is also similar to the behavior of other tools as well. When I get my license, I will need to test with larger data sets. zigGIS originally used in-memory workspaces but we noticed performance problems when loading large data sets so we switched to scratch workspaces. There are numerous ways to mitigate this but, until I can access full data sets, I can't really test it.</p>

<p>Editing worked well. I was even able to use PgMap to do further testing on my <a href="http://blog.geomusings.com/2011/08/01/triggered-notifications-using-postgis/">notification system</a>.</p>

<p>All in all, I think PgMap is an impressive tool that should be able to support the needs of ArcGIS 9.x and 10.0 users going forward. It's good to know that a tool is out there to continue meeting that demand.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Triggered Notifications Using PostGIS]]></title>
    <link href="http://blog.geomusings.com/2011/08/01/triggered-notifications-using-postgis/"/>
    <updated>2011-08-01T00:00:00-04:00</updated>
    <id>http://blog.geomusings.com/2011/08/01/triggered-notifications-using-postgis</id>
    <content type="html"><![CDATA[<p>My project work the last few months has kept me away from a lot of my favorite open-source tools and I was starting to get hives. Specifically, it had been a while since I had worked with <a href="http://www.postgresql.org">PostgreSQL</a> and <a href="http://postgis.refractions.net">PostGIS</a> and I was missing the experience, so I dreamed up something to do.</p>

<p><img alt="" class="aligncenter size-full wp-image-1898" height="307" src="http://geobabble.files.wordpress.com/2011/08/elephant.png" title="Elephant" width="461" /></p>

<p>I do a lot of work implementing situational awareness systems for my customers and one common requirement is automated notification of events. I decided that I wanted to roll a completely FOSS approach to sending an SMS notification based upon the results of a spatial query. This post will discuss the basic wiring to make it all work. I'll probably add more advanced features in subsequent posts but I'll be sticking to the basics for now.</p>

<!--more-->


<p>I decided to keep my first pass relatively simple in order to work out the core logic and workflow. To get started, I really only needed a few things:</p>

<ol>
    <li>PostgreSQL with PostGIS installed. I simply used the <a href="http://www.opengeo.org">OpenGeo</a> Community Edition.</li>
    <li><a href="http://developer.postgresql.org/pgdocs/postgres/plpython.html">plpython</a> - support for writing PostgreSQL functions in <a href="http://www.python.org">Python</a></li>
    <li>Python - I used version 2.7 for this</li>
    <li>Some spatial data - I loaded a data set of the US counties to test with</li>
</ol>


<p>Using these tools, I set out to create a core workflow that would use a trigger function attached to a point data set to test, whenever a record was inserted or updated, whether the point geometry fell within a specific US county (St. Mary's County, Maryland in this case). If so, the system would send an SMS message to me. In order to do this, I had to create the following:</p>

<ol>
    <li>An empty table with a PostGIS geometry column to store the incoming points</li>
    <li>A trigger function to perform the spatial query on insert or update</li>
    <li>A trigger to fire the trigger function</li>
    <li>A function to send the SMS message</li>
    <li>Some test SQL to insert records into the point table</li>
</ol>


<p>The first step was loading my county data, which I had in shapefile format. For that, I simply used <a href="http://www.qgis.org/">QGIS</a> and its SPIT plug-in. After that, I created the table that would hold my point data. Right now, it's just a point and an ID. The SQL is very simple:</p>

<p>{% codeblock lang:postgres %}
CREATE TABLE locations (gid SERIAL PRIMARY KEY);
SELECT AddGeometryColumn ('public','locations','shape',4326,'POINT',2);
{% endcodeblock %}</p>

<p>This table is the one that does all the work. The initial trigger will be attached to it. The counties data set really just sits there waiting to be queried. I'll actually take the next three functions in the reverse of the order in which they will execute.</p>

<p>The first function I wrote was the one that sends off the actual SMS message. That is its sole job in this process. After looking at different ways to accomplish the SMS, my old <a href="http://www.obtusesoft.com/">zigGIS</a> partner, <a href="http://twitter.com/xanadont">Abe</a> <a href="http://www.linkedin.com/in/agillesp">Gillespie</a>, clued me in to using each provider's e-mail gateway. So, by sending a properly formatted e-mail, the end user will receive an SMS message. You can learn more about it <a href="http://www.emailtextmessages.com/">here</a>. (Thanks, Abe!) So, my task really just became sending an e-mail message, which is nice because I want support that as well so now I can reuse code. After looking at various means to send e-mail from within PostgreSQL, I decided that Python was the most direct way to accomplish it.</p>

<p>PostgreSQL supports using Python for functions in a manner similar to the way SQL Server supports embedded procedures written against the <a href="http://en.wikipedia.org/wiki/Common_Language_Runtime">Common Language Runtime</a>. You simply need to make sure plpython support is installed with your instance of PostgreSQL. Here is the code for the simplified messaging function:</p>

<p>{% codeblock lang:postgres %}
CREATE OR REPLACE FUNCTION emailme(txt text)
  RETURNS integer AS
$BODY$</p>

<h1>python starts here</h1>

<h1>Import smtplib for the actual sending function</h1>

<p>import sys
import smtplib
import email
import re</p>

<h1>Import the email modules we'll need</h1>

<p>from email.mime.text import MIMEText
from threading import Thread</p>

<p>def mailfunction(recip,msgtxt,*args):</p>

<pre><code>msg = MIMEText(msgtxt)

# me == the sender's email address
# you == the recipient's email address
msg['Subject'] = 'Message from PostgreSQL'
msg['From'] = 'contact@zekiah.com'
#msg['To'] = 'not used here'

# Send the message via our own SMTP server, but don't include the
# envelope header.
s = smtplib.SMTP('localhost')
s.sendmail('contact@zekiah.com', [recip], msg.as_string())
s.quit()
</code></pre>

<p>t = Thread(target=mailfunction,args=('1234567890@vtext.com', txt))
t.run()
return 0
$BODY$
  LANGUAGE plpythonu VOLATILE
  COST 100;
ALTER FUNCTION emailme(text) OWNER TO postgres;
{% endcodeblock %}</p>

<p>As can be seen, the Python code is embedded in the body of the function. Early on, I was experiencing significant slowness when sending the messages. After some code refactoring, I realized it was the SMTP handshake that was causing the problem. The original version used an external server that required authentication. I installed a local open-source SMTP server and configured it to allow unauthenticated relay from the local server and the bottleneck went away.</p>

<p>This version of the code receives the message body as a parameter and sends to a hard-coded address. I plan to change this to accept the recipient address as well. Then the calling function can pass in an array of recipients who have subscribed to these messages. For now, the system just talks to me.</p>

<p>Next, I built the trigger function. This is a little different from other platforms I've worked on. In SQL Server, a trigger is basically a stored procedure that gets called when a table event happens. When authoring, you simply build the one procedure. In PostgreSQL, you have two distinct objects: a trigger and a trigger function. The trigger function does the heavy lifting and the trigger is what calls it. In my case, the trigger function is what performs the actual spatial query to determine if the new point geometry falls within my county. The code for that function is here:</p>

<p>{% codeblock lang:postgres %}
CREATE OR REPLACE Function checkcounty() RETURNS TRIGGER AS</p>

<p>$BODY$
DECLARE</p>

<p>shp geometry;
a_row counties%ROWTYPE;</p>

<p>BEGIN
shp = new.shape;</p>

<p>SELECT * FROM counties WHERE ST_Contains(counties.shape, shp) INTO a_row;</p>

<p>IF a_row IS NOT NULL THEN</p>

<pre><code>IF a_row."COUNTY" = 'Saint Marys County' THEN
    PERFORM emailme('New feature in St. Mary`s County');
END IF;
</code></pre>

<p>END IF;
RETURN new;</p>

<p>END;
$BODY$
LANGUAGE 'plpgsql' VOLATILE;
{% endcodeblock %}</p>

<p>Again, I'm just checking for one, hard-coded value. In my next iteration, I plan to make that configurable so that the trigger function will test for various user-specified conditions. Of course, the type of spatial relationship can also be configured.</p>

<p>Lastly, the trigger itself. As can be seen, it's primary job is to call the trigger function when appropriate.</p>

<p>{% codeblock lang:postgres %}
CREATE TRIGGER locations_change
  AFTER INSERT OR UPDATE
  ON locations
  FOR EACH ROW
  EXECUTE PROCEDURE checkcounty();
{% endcodeblock %}</p>

<p>Once these were all in place, I used the following to test:</p>

<p>{% codeblock lang:postgres %}
--TRUE
insert into locations
(shape)
VALUES
(ST_PointFromText('POINT(-76.662 38.348)', 4326));</p>

<p>--FALSE
--insert into locations
--(shape)
--VALUES
--(ST_PointFromText('POINT(-76.622 37.895)', 4326));</p>

<p>--NULL
--insert into locations
--(shape)
--VALUES
--(ST_PointFromText('POINT(-74.482 37.734)', 4326));
{% endcodeblock %}</p>

<p>These objects represent the basic skeleton of the notification capability I am building. In the future, I also want to support various types of messaging in addition to SMS and e-mail. I'm thinking of Twitter, publishing to RSS or Atom and other such options.</p>

<p>Even at this early stage, I've got to consider performance. The spatial query in the trigger is performing amazingly fast but my data is still small. I've got spatial indices built on the data sets but I'll need to keep an eye on that as this grows. Thankfully, <a href="http://www.manning.com/obe/">'PostGIS In Action'</a> offers lots of tips in that regard. Another design consideration I've made is that all of my spatial data sets will be stored in the same spatial reference. PostGIS has nice coordinate transformation capabilities but I don't want to introduce that into my trigger functions in case data starts to grow. I plan to pre-process buffers and such for the same reasons.</p>

<p>I'm pretty happy with this so far primarily because all of the logic is executing at the database level. There's really no need to extract any of this logic out into a middle-tier library of any kind and it's running without any dependence on any middleware. It's also a nice use of spatial processing that doesn't involve a map. I love maps but they aren't necessary in every application of spatial technology. I'm also excited to have a meaty requirement to dig into to help with my Python explorations. I've missed working with these tools. It's good to be back.</p>
]]></content>
  </entry>
  
</feed>
