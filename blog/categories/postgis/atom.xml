<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PostGIS | geoMusings]]></title>
  <link href="http://blog.geomusings.com/blog/categories/postgis/atom.xml" rel="self"/>
  <link href="http://blog.geomusings.com/"/>
  <updated>2014-04-17T14:50:22-04:00</updated>
  <id>http://blog.geomusings.com/</id>
  <author>
    <name><![CDATA[Bill Dollins]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Little Deeper with Node and PostGIS]]></title>
    <link href="http://blog.geomusings.com/2014/02/18/a-little-deeper-with-node-and-postgis/"/>
    <updated>2014-02-18T13:44:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/18/a-little-deeper-with-node-and-postgis</id>
    <content type="html"><![CDATA[<p>What does one do when presented with more snow days than expected? My friends in Colorado would probably do something outrageous like skiing, but I found it to be a great opportunity to catch up on some of my recreational coding. Specifically, I wanted to revisit the <a href="http://blog.geomusings.com/2013/12/11/building-a-simple-geodata-service-with-node-and-amazon-rds/">Node/PostGIS work I blogged about earlier</a>.</p>

<p>As fun as that project was, it was fairly limited in scope and I wanted to spread my wings a little more with Node. So I decided to build a more general-purpose wrapper around <a href="http://postgis.net">PostGIS</a>. Lately, I've become a bit obsessed with the idea that PostGIS may be the only GIS tool you really need in terms of data storage, management, and analytics. That's probably a topic for another post but exploring that concept was a perfect premise for my continued explorations with Node.</p>

<p>I have been circling back to Node over the past few months to continue building my comfort level with it. I tend to eschew frameworks when i have learning something new because I want to get comfortable with the core before I start layering on abstraction. That was my approach with <a href="http://blog.geomusings.com/2013/04/25/simple-tile-viewer/">the tile viewer tool I built a while back</a>. For the recent post centered on Amazon RDS, I added Express into the mix, which has been a big help.</p>

<p>This time around, I wanted to dig a little deeper with the <a href="https://github.com/brianc/node-postgres">node-postgres</a> module and also make the application more modular. I wanted to build around a few core principles:</p>

<ol>
<li>Keep it RESTful (or as close to it as I could)</li>
<li>GeoJSON in/GeoJSON out (so....vector only for now)</li>
<li>Let PostGIS do the heavy lifting</li>
</ol>


<!--more-->


<p><strong>Getting Started</strong></p>

<p>This time around, I elected to use my local PostgreSQL/PostGIS instance rather than Amazon RDS. This was mainly so I could keep my development isolated on one machine. I already had the basic infrastructure in place from my last time around, so I was able to quickly dive into the meat of things. I decided to scope my initial effort at the following:</p>

<ol>
<li>Return the contents of an entire table as GeoJSON, with the ability to choose between features (geometries and attributes) in a GeoJSON feature collection or just geometries in a GeoJSON geometry collection. This should support any table in the database.</li>
<li>Return those records in a table that intersect a geometry passed in as a parameter. The input geometry would be in GeoJSON format.</li>
<li>Return a JSON representation of a table's schema.</li>
<li>Return a list of tables from the server. This is necessary in order to support the ability to query any available table.</li>
<li>Implement some simple, but application-specific logic to demonstrate extensibility.</li>
</ol>


<p>With these goals in mind, I decided to first tackle the issue of extensibility. I wanted to make it as painless as possible and <a href="http://timstermatic.github.io/blog/2013/08/17/a-simple-mvc-framework-with-node-and-express/">the strategy described in this post</a> seemed to fit the bill. I just had to add the following code block to my server.js (straight from the post):</p>

<p>{% codeblock snippet1.js %}
// dynamically include routes (Controller)
fs.readdirSync('./controllers').forEach(function (file) {
  if(file.substr(-3) == '.js') {</p>

<pre><code>  route = require('./controllers/' + file);
  route.controller(app);
</code></pre>

<p>  }
});
{% endcodeblock %}</p>

<p>This will load any .js file in the controllers directory into the application. If they are written to the pattern expected by Express, new resource paths are exposed to the application. The post above describes a simple MVC implementation. Astute readers will note that my take is all "C" without "M" or "V." I plan to refactor that later but it it was easier for me to keep track of things on this pass with code in one place.</p>

<p><strong>Getting Data</strong></p>

<p>With modularity out of the way, it was time work on the basic structure for getting data from the database. In <a href="https://github.com/geobabbler/node-gis-server/blob/master/controllers/core.js">core.js</a>, I defined a route with a URL template like '/vector/:schema/:table/:geom'. This would translate into something like http://localhost/vector/public/parcels/features, which would fetch a GeoJSON feature collection containing the contents of the parcels table. To do that, I need to know the name of the spatial column in the table, which the following helps me retrieve:</p>

<p>{% codeblock snippet2.js %}
var meta = client.query("select * from geometry_columns where f_table_name = '" + tablename + "' and f_table_schema = '" + schemaname + "';");
{% endcodeblock %}</p>

<p>The next code block shows how I capture the name of the spatial column and structure the main query, depending on the choice of features or geometry:</p>

<p>{% codeblock snippet3.js %}
meta.on('row', function (row) {</p>

<pre><code>var query;
var coll;
spatialcol = row.f_geometry_column;
if (geom == "features") {
    query = client.query("select st_asgeojson(st_transform(" + spatialcol + ",4326)) as geojson, * from " + fullname + ";");
    coll = {
        type : "FeatureCollection",
        features : []
    };
} else if (geom == "geometry") {
    query = client.query("select st_asgeojson(st_transform(" + spatialcol + ",4326)) as geojson from " + fullname + ";");
    coll = {
        type : "GeometryCollection",
        geometries : []
    };
}
</code></pre>

<p>//'meta' code block continues
{% endcodeblock %}</p>

<p>As can be seen above, the query will transform the output geometry to WGS84 and convert it to GeoJSON for me. So I'm sticking my third principle by leaning on PostGIS functions here. I plan to stick to GeoJSON's default spatial reference of WGS84 for now. To roll up the query results into the appropriate GeoJSON object and return it, I handled the 'row' and 'end' events.</p>

<p>{% codeblock snippet4.js %}
//roll up the results
query.on('row', function (result) {</p>

<pre><code>if (!result) {
    return res.send('No data found');
} else {
    if (geom == "features") {
        coll.features.push(geojson.getFeatureResult(result, spatialcol)); //use helper function
    } else if (geom == "geometry") {
        var shape = JSON.parse(result.geojson);
        coll.geometries.push(shape);
    }
}
</code></pre>

<p>});</p>

<p>//send the results
query.on('end', function (err, result) {</p>

<pre><code>res.setHeader('Content-Type', 'application/json');
res.send(coll);
</code></pre>

<p>});
{% endcodeblock %}</p>

<p>I wrote a helper function to roll up GeoJSON features:</p>

<p>{% codeblock snippet5.js %}
exports.getFeatureResult = function(result, spatialcol) {</p>

<pre><code>    var props = new Object;
    var crsobj = {
        "type" : "name",
        "properties" : {
            "name" : "urn:ogc:def:crs:EPSG:6.3:4326"
        }
    };
    //builds feature properties from database columns
    for (var k in result) {
        if (result.hasOwnProperty(k)) {
            var nm = "" + k;
            if ((nm != "geojson") &amp;&amp; nm != spatialcol) {
                props[nm] = result[k];
            }
        }
    }

    return {
        type : "Feature",
        crs : crsobj,
        geometry : JSON.parse(result.geojson),
        properties : props
    };
};
</code></pre>

<p>{% endcodeblock %}</p>

<p>So that's basic data retrieval. How about that spatial intersect?</p>

<p><strong>A Simple Spatial Query</strong></p>

<p>One thing I failed to mention in the above section, is that all of that is exposed through an HTTP GET request. For this next function, I'm going to use a POST. I went back and forth on that but came down on the side of POST due to the potential for a user to send a very verbose input shape. The function is designed to accept JSON as the body of the request, which would be done in curl like this:</p>

<p>{% codeblock snippet6.bat %}
curl -X POST -d "{ \"type\": \"Point\", \"coordinates\": [-98.35, 39.7] }" -H "Content-Type: application/json" http://localhost:3000/vector/public/states_gen/features/intersect
{% endcodeblock %}</p>

<p>The above action returns the state of Kansas (I knew you were wondering). To make this happen, there are only three things that are different. First, the URL is defined a POST and, second, the code needs to capture the input shape. The first few lines are:</p>

<p>{% codeblock snippet7.js %}</p>

<pre><code>/**
 * retrieve all features that intersect the input GeoJSON geometry
 */
app.post('/vector/:schema/:table/:geom/intersect', function (req, res, next) {
    //console.log(JSON.stringify(req.body));
    var queryshape = JSON.stringify(req.body);
//continue with the rest of app.post
</code></pre>

<p>{% endcodeblock %}</p>

<p>I stringify the JSON since I have to insert it into my SQL. This brings me to the third difference here, the query. This time, I am using ST_INTERSECTS to filter down the response. So, depending on the choice of features or geometry, the query will be similar to:</p>

<p><strong>"select st_asgeojson(st_transform(" + spatialcol + ",4326)) as geojson, * from " + fullname + " where ST_INTERSECTS(" + spatialcol + ", ST_SetSRID(ST_GeomFromGeoJSON('" + queryshape + "'),4326));"</strong></p>

<p>The rest of the process is similar to the basic query above. With a well-exercised data access pattern in place, querying table schema and layer lists become trivial. Since GeoJSON doesn't cover these topics, I had to roll my own. I won't detail the output but the queries are below.</p>

<p>{% codeblock snippet8.js %}
//SQL to retrieve schema
//var sql = "SELECT n.nspname as schemaname,c.relname as table_name,a.attname as column_name,format_type(a.atttypid, a.atttypmod) AS //type,col_description(a.attrelid, a.attnum) as comments";
//sql = sql + " FROM pg_class c INNER JOIN pg_namespace n ON c.relnamespace = n.oid LEFT JOIN pg_attribute a ON a.attrelid = c.oid";
//sql = sql + " WHERE a.attnum > 0 and c.relname = '" + tablename + "' and n.nspname = '" + schemaname + "';";</p>

<p>//SQL to retrieve layer list
//sql = "SELECT 'geometry' AS geotype, * FROM geometry_columns UNION SELECT 'geography' as geotype, * FROM geography_columns;";
{% endcodeblock %}</p>

<p>So this gives me everything I need for an all-purpose interface into PostGIS from Node. I could spend the rest of the year similarly wrapping the hundreds of spatial functions in PostGIS but the real power of extensibility is the ability to tailor an application for one's own needs, based upon one's detailed understanding of their own data and logic.</p>

<p><strong>Adding Some Customization</strong></p>

<p>To do this, I fell back to the data for <a href="http://leonardtown.somd.com">Leonardtown, Maryland</a> that have used in a <a href="http://blog.geomusings.com/2013/06/18/geojson-on-github-now-what/">couple</a> of previous <a href="http://blog.geomusings.com/2011/10/13/cartodb-leaflet-easy/">posts</a>. I am simply going to expose the ability to query residential or commercial buildings from the data set. For this, all of the prep work is done at the top of the function by simply preparing a WHERE clause.</p>

<p>{% codeblock snippet9.js %}
app.get('/leonardtown/buildings/:geom', function (req, res, next) {</p>

<pre><code>var client = new pg.Client(app.conString);
var geom = req.params.geom.toLowerCase();
if ((geom != "features") &amp;&amp; (geom != "geometry")) {
    res.status(404).send("Resource '" + geom + "' not found");
    return;
}
var tablename = "leonardtown_bldgs";
var schemaname = "public";
var fullname = schemaname + "." + tablename;
var spatialcol = "";
var proptype = req.query.type;
var whereclause = ";";
if (typeof proptype != "undefined") {
    if (proptype.toLowerCase() != "all") {
        whereclause = " where structure_ = '" + proptype + "';";
    }
}
var coll;
var sql = "";
//logic continues from here
</code></pre>

<p>{% endcodeblock %}</p>

<p>The primary difference here are that I am using a GET with a query string since I'm not concerned with data size and that I'm building a WHERE clause on a specific column name. What's not shown is that, farther down, I don't need to query the name of the spatial column so I can cut out that step. I can do this because I understand my own data so I can be more succinct that if I were writing a more generic function. Using this approach I can also write more complex custom logic in my database, call it from Node, and send the response. In other words, standard web application behavior.</p>

<p>In order to expose this application-specific logic, I just needed expose it in a separate leonardtown.js file and drop it into the 'controllers' directory.</p>

<p><strong>Wrapping Up</strong></p>

<p>This post was bit longer than usual but there was lot of ground to cover. I feel like I'm getting more comfortable with the Node ecosystem though I'm still a bit wobbly. My next step is probably to dive a little deeper into the MVC side of things with something like <a href="http://sailsjs.org">Sails</a>. Having a familiar face like PostGIS on the back end is helping me as I figure out how to perform more meaningful tasks with Node and its related tools.</p>

<p>If you want to check out the full code for this application, it is here: <a href="https://github.com/geobabbler/node-gis-server">https://github.com/geobabbler/node-gis-server</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a Simple Geodata Service with Node, PostGIS, and Amazon]]></title>
    <link href="http://blog.geomusings.com/2013/12/11/building-a-simple-geodata-service-with-node-and-amazon-rds/"/>
    <updated>2013-12-11T15:18:00-05:00</updated>
    <id>http://blog.geomusings.com/2013/12/11/building-a-simple-geodata-service-with-node-and-amazon-rds</id>
    <content type="html"><![CDATA[<p><strong>tl;dr</strong></p>

<p>This post describes the construction of a simple, lightweight geospatial data service using Node.JS, PostGIS and Amazon RDS. It is somewhat lengthy and includes a number of code snippets. The post is primarily targeted at users who may be interested in alternative strategies for publishing geospatial data but may not be familiar with the tools discussed here. This effort is ongoing and follow-up posts can be expected.</p>

<p><strong>Rationale</strong></p>

<p>I'm always looking for opportunities to experiment with new tools and the announcement of PostgreSQL/PostGIS support on Amazon RDS piqued my curiosity. Over the past six months, I have run into the repeated need on a couple of projects to be able to get the bounding box of various polygon features in order to drive dynamic mapping displays. Additionally, the required spatial references of these projects have varied beyond WGS84 and Web Mercator.</p>

<p>With that, the seeds of a geodata service were born. I decided to build one that would, via a simple HTTP call, return the bounding box of a polygon or the polygon itself, in the spatial reference of my choice as a single GeoJSON feature.</p>

<p>I knew I wanted to use PostGIS hosted on Amazon RDS to store my data. Here are the rest of the building blocks for this particular application:</p>

<ol>
<li><a href="http://nodejs.org">Node.js</a></li>
<li>Express web application framework for Node</li>
<li>PG module for accessing PostgreSQL with Node</li>
<li>Natural Earth 1:10M country boundaries</li>
</ol>


<!--more-->


<p><strong>Setting up PostGIS on Amazon RDS</strong></p>

<p>Setting up the PostgreSQL instance on RDS was very easy. I simply <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreatePorstgreSQLInstance.html">followed the instructions here</a> for doing it in the AWS Management Console. I also got a lot of use out of <a href="http://www.databasesoup.com/2013/11/first-look-at-postgresql-rds-on-amazon.html">this post by Josh Berkus</a>. Don't forget to also set up your security group to govern access to your database instance <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithSecurityGroups.html">as described here</a>. I prefer to grant access to specific IP addresses.</p>

<p>Now that the Amazon configuration is done, your RDS instance essentially behaves the same as if you had set it up on a server in your server room. You can now access the instance using all of the standard PostgreSQL tools with which you are familiar. This is good because we need to do at least one more thing before we load our spatial data: we have to enable the PostGIS extension. I find that it is easiest to accomplish this at the command line:</p>

<p><em>psql -U {username} -h {really long amazon instance host name} {database name}</em></p>

<p>Once you've connected, issue the command to enable PostGIS in your database:</p>

<p><em>CREATE EXTENSION postgis;</em></p>

<p>You may also want to enable topology while you're here:</p>

<p><em>CREATE EXTENSION postgis_topology;</em></p>

<p>This should complete your setup. Now you are ready to load data.</p>

<p><strong>Loading Spatial Data</strong></p>

<p>As I mentioned above, we are now dealing with a standard PostgreSQL server that happens to be running on Amazon RDS. You can use whatever workflow you prefer to load your spatial data.</p>

<p style="text-align:center;"><img src="http://blog.geomusings.com/images/posts/pgadmin_rds.png" /></p>

<p>I downloaded the <a href="http://www.naturalearthdata.com/downloads/10m-cultural-vectors/">Natural Earth 1:10M country polygons</a> for this effort. Once downloaded, I used the DB Manager extension to <a href="http://qgis.org">QGIS</a> to import the data to PostgreSQL. I also did a test import with <a href="http://www.gdal.org/ogr/">OGR</a>. Both worked fine so it's really a matter of preference.</p>

<p><strong>Building the Application</strong></p>

<p>I chose to use Node.js because it is very lightweight and ideal for building targeted web applications. I decided to use the <a href="http://expressjs.com/">Express web framework for Node</a>, mainly because it makes things very easy. To access PostgreSQL, I used the <a href="https://github.com/brianc/node-postgres">node-postgres module</a>. I was planning to deploy the application in an Ubuntu instance on Amazon EC2, so I chose to do the development on Ubuntu. Theoretically, that shouldn't matter with Node but the node-postgres module builds a native library when it is installed so it was a factor here.</p>

<p>After building the package.json file and using that to install the Express, node-postgres, and their dependencies, I build a quick server script to act as the web interface for the application. This is where Express really excels in that it makes it easy to define resource paths in an application.</p>

<p>{% codeblock server.js %}
var express = require('express'),</p>

<pre><code>geo = require('./routes/geo');
</code></pre>

<p>var app = express();</p>

<p>app.get('/countries/:id/bbox', geo.bbox);
app.get('/countries/:id/bbox/:srid', geo.bboxSrid);
app.get('/countries/:id/polygon', geo.polygon);
app.get('/countries/:id/polygon/:srid', geo.polygonSrid);</p>

<p>app.listen(3000);
console.log('Listening on port 3000...');
{% endcodeblock %}</p>

<p>The four "app.get" statements above define calls to get either the bounding box or the actual polygon for a country. When the ":srid" parameter is not specified, the resulting feature is returned in the default spatial reference of WGS84. If a valid EPSG spatial reference code is supplied, then the resulting feature is transformed to that spatial reference. The ":id" parameter in all of the calls represents the ISO Alpha-3 code for a country. You will notice that the application listens on port 3000. More on that later.</p>

<p>The next step is to define the route handlers. In this application, this where interaction with PostGIS will take place. Note that each of the exports correspond to the callback functions in the app.get statements above.</p>

<p>{% codeblock geo.js %}
var pg = require('pg');
var conString = "postgres://username:password@hostname.rds.amazonaws.com:5432/database"; //TODO: point to RDS instance</p>

<p>exports.bbox = function(req, res) {</p>

<pre><code>var client = new pg.Client(conString);
client.connect();
var crsobj = {"type": "name","properties": {"name": "urn:ogc:def:crs:EPSG:6.3:4326"}};
var idformat = "'" + req.params.id + "'";
idformat = idformat.toUpperCase();  
var query = client.query("select st_asgeojson(st_envelope(shape)) as geojson from ne_countries where iso_a3 = " + idformat + ";"); 
var retval = "no data";
query.on('row', function(result) {
    client.end();
    if (!result) {
      return res.send('No data found');
    } else {
      res.setHeader('Content-Type', 'application/json');
      //build a GeoJSON feature and return it
      res.send({type: "feature",crs: crsobj, geometry: JSON.parse(result.geojson), properties:{"iso": req.params.id, "representation": "extent"}});
    }
  }); 
</code></pre>

<p>};</p>

<p>exports.bboxSrid = function(req, res) {</p>

<pre><code>var client = new pg.Client(conString);
client.connect();
var crsobj = {"type": "name","properties": {"name": "urn:ogc:def:crs:EPSG:6.3:" + req.params.srid}};
var idformat = "'" + req.params.id + "'";
idformat = idformat.toUpperCase();  
var query = client.query("select st_asgeojson(st_envelope(st_transform(shape, " + req.params.srid + "))) as geojson from ne_countries where iso_a3 = " + idformat + ";"); 
var retval = "no data";
query.on('row', function(result) {
    client.end();
    if (!result) {
      return res.send('No data found');
    } else {
      res.setHeader('Content-Type', 'application/json');
      res.send({type: "feature",crs: crsobj, geometry: JSON.parse(result.geojson), properties:{"iso": req.params.id, "representation": "extent"}});
    }
  }); 
</code></pre>

<p>};</p>

<p>exports.polygon = function(req, res) {</p>

<pre><code>//TODO: Flesh this out. Logic will be similar to bounding box.
var client = new pg.Client(conString);
client.connect();
var crsobj = {"type": "name","properties": {"name": "urn:ogc:def:crs:EPSG:6.3:4326"}};
var idformat = "'" + req.params.id + "'";
idformat = idformat.toUpperCase();  
var query = client.query("select st_asgeojson(shape) as geojson from ne_countries where iso_a3 = " + idformat + ";"); 
var retval = "no data";
query.on('row', function(result) {
    client.end();
    if (!result) {
      return res.send('No data found');
    } else {
      res.setHeader('Content-Type', 'application/json');
      res.send({type: "feature", crs: crsobj, geometry: JSON.parse(result.geojson), properties:{"iso": req.params.id, "representation": "boundary"}});
    }
  }); };
</code></pre>

<p>exports.polygonSrid = function(req, res) {</p>

<pre><code>var client = new pg.Client(conString);
client.connect();
var crsobj = {"type": "name","properties": {"name": "urn:ogc:def:crs:EPSG:6.3:" + req.params.srid}};
var idformat = "'" + req.params.id + "'";
idformat = idformat.toUpperCase();  
var query = client.query("select st_asgeojson(st_transform(shape, " + req.params.srid + ")) as geojson from ne_countries where iso_a3 = " + idformat + ";"); 
var retval = "no data";
query.on('row', function(result) {
    client.end();   
    if (!result) {
      return res.send('No data found');
    } else {
      res.setHeader('Content-Type', 'application/json');
      res.send({type: "feature",crs: crsobj, geometry: JSON.parse(result.geojson), properties:{"iso": req.params.id, "representation": "boundary"}});
    }
  }); };
</code></pre>

<p>{% endcodeblock %}</p>

<p>The PostGIS spatial SQL for each function is shown in the "client.query" calls in the code above. This approach is very similar to constructing SQL calls in a number of other application environments. You will notice that a coordinate reference system object is constructed and attached to each valid response, which is structured as a <a href="http://geojson.org">GeoJSON</a> feature. The code currently assumes EPSG codes but that may be addressed in a future version.</p>

<p>The above modules do most of the heavy lifting. The full code for this sample <a href="https://github.com/geobabbler/geo-service">is available here</a>.</p>

<p>To test the application, simply issue the following command in a terminal:</p>

<p><em>node server.js</em> (this assumes you are running from the same directory in which server.js resides. The file extension is optional.</p>

<p>Your web application is now listening on port 3000. In a browser, visit the following URL:</p>

<p><em>http://localhost:3000/countries/irl/bbox</em></p>

<p>This should return a GeoJSON feature representing the bounding box of Ireland in WGS84. You can then test the other three calls with appropriate parameters. To get the bounding box in Web Mercator, go to:</p>

<p><em>http://localhost:3000/countries/irl/bbox/3785</em></p>

<p><strong>Deploying the Application</strong></p>

<p>The application should now be ready to deploy. In my case, I created an Ubuntu EC2 instance (free tier). Using SSH, I made sure Node and git were installed on the machine. Additionally, I installed <a href="https://github.com/nodejitsu/forever">Forever</a> which allows a Node application to run continuously (similar to a service on Windows). This can also be done using an upstart script but I chose Forever. I may switch to using <a href="http://devo.ps/blog/2013/06/26/goodbye-node-forever-hello-pm2.html">PM2</a> at some point.</p>

<p>Now, it's simply matter of installing the application code to the instance via git, wget, or the method of your choice. Once installed, be sure to go to the folder containing the code and issue the "npm install" command. This will read the package.json install Express, node-postgres, and other dependencies. Since some native code is built in the process, you may need to issue the command under sudo.</p>

<p>I mentioned above that the application listens on port 3000. The Ubuntu instance, by default, will not allow the application to listen on port 80. This can be mitigated in a number of ways but I issued the following command to redirect traffic from 80 to 3000. Since this instance is single-use, this approach is sufficient.</p>

<p><em>sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-ports 3000</em></p>

<p>Once you are ready to go, you'll want to start the application with the following command:</p>

<p><em>forever start server</em> (again assuming you are executing from the directory containing server.js)</p>

<p>A couple of Amazon notes: 1) You may want to assign an elastic IP to your instance for a persistent IP address and 2) you'll want you remember to configure your RDS security group to allow access from your instance's IP address.</p>

<p><strong>Conclusion</strong></p>

<p>If everything has gone correctly, you should be able to execute the above URLs (using your instance IP address) and get a response like the following, which you should be able to load directly into QGIS or another GeoJSON-literate client. Altogether, I was able to assemble this in one evening. This small collection of open-source tools, combined with the Amazon infrastructure, seems to provide a straightforward path to a hosted geodata service. This example is intentionally simple but PostGIS provides a robust collection of functions that can be exploited in a similar manner, leading to more advanced processing or analysis. I will continue my experimentation but am encouraged by what I have seen so far.</p>

<p><strong>Sample Response</strong></p>

<p>{% codeblock irl_bbox.json %}
{
  "type": "feature",
  "crs": {</p>

<pre><code>"type": "name",
"properties": {
  "name": "urn:ogc:def:crs:EPSG:6.3:4326"
}
</code></pre>

<p>  },
  "geometry": {</p>

<pre><code>"type": "Polygon",
"coordinates": [
  [
    [
      -10.4781794909999,
      51.4457054710001
    ],
    [
      -10.4781794909999,
      55.386379299
    ],
    [
      -5.99351966099994,
      55.386379299
    ],
    [
      -5.99351966099994,
      51.4457054710001
    ],
    [
      -10.4781794909999,
      51.4457054710001
    ]
  ]
]
</code></pre>

<p>  },
  "properties": {</p>

<pre><code>"iso": "irl",
"representation": "extent"
</code></pre>

<p>  }
}
{% endcodeblock %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Consider the 'Alternative']]></title>
    <link href="http://blog.geomusings.com/2013/11/27/consider-the-alternative/"/>
    <updated>2013-11-27T13:01:00-05:00</updated>
    <id>http://blog.geomusings.com/2013/11/27/consider-the-alternative</id>
    <content type="html"><![CDATA[<p>When I was in college, I had a psychology professor who posited that you could train a cat (a dodgy proposition at best) to take a circuitous route to its food bowl by only rewarding that behavior. He was clearly a behaviorist and was convinced that you could completely condition the instinct to go straight to the food bowl out of the cat. To my knowledge, this professor did not own a cat and never attempted to test his assertion.</p>

<p>I was reminded of this after reading my friend Atanas Entchev's <a href="http://blog.entchev.com/2013/11/22/some-post-postgis-day-thoughts.aspx">post in reaction</a> to the <a href="http://postgis.net">PostGIS</a> Day <a href="http://www.spatiallyadjusted.com/2013/11/21/todays-hangout-postgis-day-extravaganza-panel/">hangout panel discussion</a>. In his post, <a href="http://twiiter.com/atanas">Atanas</a> describes difficulty in convincing customers to consider open-source geospatial tools. These customers and prospects are comfortable with their proprietary tools and associated workflows and are reluctant to consider switching. I have encountered this attitude many times myself so I take no issue with the observation. Barriers to exit are real considerations, regardless of the new technology being considered. Organizations align themselves around their tools to achieve maximum efficiency with them. I discussed these issues at a talk I gave last year to the <a href="https://njgin.state.nj.us/OIT_NJGF/index.jsp">New Jersey Geospatial Forum</a> about how organizations can extend their existing geospatial technology investments with open-source technologies. These issues are very real for any organization with a mature, extended investment in a particular technology stack.</p>

<p>Atanas went on to liken the attitude to that with which some people view alternative medicine and I can see his point. Traditional GIS has set itself apart from the rest of the technology world for so long that users are generally conditioned to believe that GIS workflows should involve a series of <a href="http://en.wikipedia.org/wiki/Rube_Goldberg">Rube Goldberg</a> machinations involving file-based data sets, some proprietary scripting, and possibly some application-level business logic to relate and/or join data as necessary. This has taken various forms over the years but diagrams of those workflows tend to look the same.</p>

<p style="text-align:center;"><img src="http://blog.geomusings.com/images/posts/geo_model.png" /></p>

<!--more-->


<p>Standing in contrast to such things, PostGIS looks alien, or "alternative." In truth, it is not "alternative" but rather "standard." As an example, here is <a href="http://blog.geomusings.com/assets/demos/nbi/">a map I produced a few weeks ago</a> showing the average ages of bridges by county. (I am not a cartographer.) It is a simple aggregation of the <a href="http://www.fhwa.dot.gov/bridge/nbi.cfm">National Bridge Inventory</a>, which consists of tens of thousands of records by county (3100-ish records). All of the data processing was done in PostgreSQL/PostGIS using nothing more exotic than SQL aggregate functions and some joins. None of the operations took longer than 6 seconds on my very pedestrian laptop. When I was done, I used QGIS to play with visualization and then dump out the static GeoJSON for use in Leaflet.</p>

<p>For my many friends who are regular users of PostGIS, this is nothing exotic. For some of my friends who regularly use commercial tools, this is interesting but not earth-shattering. But for a large portion of my friends who are comfortable with traditional tools and workflows, the time-to-market for this effort (35 minutes from the time I downloaded the NBI to the time I pushed the map to GitHub) has them taking notice. This entire workflow involved SQL extended with OGC-compliant spatial objects. (Side note: I have been hard on OGC's web efforts but the Simple Features Specification has been a quiet workhorse across the geospatial industry for over a decade. It's a good example of the benefit that well-designed standards can provide.) The map is being served from static content over simple HTTP with some client-side Javascript handling visualization. No heavy APIs or middleware involved or needed. The QGIS part was really necessitated by own cartographic limitations, but I could have fully generated the GeoJSON from SQL as well.</p>

<p>This example is fairly simplistic but I have good friends that are using PostGIS, and nothing more, to perform analyses and produce results for decision makers while sitting in meetings. This type of turnaround is standard in other market segments and the geospatial industry should expect nothing less. It requires nothing more than a strong foundation in SQL, mastery of spatial processes, and detailed understanding of your own data.</p>

<p>So I have come to realize that the mainstream GIS community has become very much like my professor's theoretical cat; conditioned to take the long way to the end result when more direct paths are clearly available. What's more, they have become conditioned to think of such approaches as normal. Geospatial analytical operations can be very complex and the approaches to performing them were, in the past, necessarily convoluted due to the lack of understanding of spatial data types and operations within mainstream platforms. Those barriers have been rapidly disappearing over the past decade or so, but the user community has been slow to let go of its comfort with familiar tools and convoluted approaches. As I stated above, organizational barriers to exit are real considerations, but the inherent efficiencies available in modern geospatial tools such as PostGIS make the transition worth the effort.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GIS Day After]]></title>
    <link href="http://blog.geomusings.com/2013/11/19/gis-day-after/"/>
    <updated>2013-11-19T09:13:00-05:00</updated>
    <id>http://blog.geomusings.com/2013/11/19/gis-day-after</id>
    <content type="html"><![CDATA[<p>It's the morning of November 21st, but not for long. You open one eye. Just one; it's best not to rush such things. Apparently, you finally came to rest in the ball pit you all made using the squishy globes from myriad conferences past. A cursory scan tells you the GIS lab is trashed. It starts to come back to you: the rousing game of "Pin the Certificate on the Khakis." Yes, there are your pleated khakis on the wall with everyone's training and GISP certificates stuck on or around them with pushpins. Someone won in what would have been a most painful way if the khakis had been on your body. The loin cloth fashioned from the old hard-copy topos (which you are still wearing). The fact that you let the intern talk you into finally opening a Twitter account and your glee at discovering you could attach photos to geocoded tweets with your BlackBerry.</p>

<p>You look around the room at your coworkers strewn across the floor. There's the ArcObjects guy still sporting his "war paint" from the plotter toner. There's your lead analyst with a face tattoo of the town's land-use drawn in marker and, she proudly insisted, accurately projected in state plane. Slowly, you are able to account for everyone on your GIS staff. Good, no one has gone missing...except the intern.</p>

<p>The door opens and you turn to see the intern standing there in all of her college-kid resilience, letting in far too much sunlight for your comfort. You're not sure it's possible to hate anyone more in this moment.</p>

<p>"Oh, good," she says, "you're awake. Are you going to do the hangout?"</p>

<p>"Hangout?" you mumble.</p>

<p>"Yeah, <a href="http://www.spatiallyadjusted.com/2013/11/18/postgis-day-special-hangout-this-week/">James Fee is doing a PostGIS Day hangout with a panel of experts on PostGIS</a>. I told you about this. I told you about <a href="http://postgis.net">PostGIS</a>. Don't you remember? It starts in about an hour."</p>

<p>It does sound familiar. You give it some thought. What better way to shake off the cobwebs from the bacchanalia of the "BEST GIS DAY EVAR!!!" (so says the hand-lettered Sharpie on the lab's wall) than a little geo-hair-of-the-dog? Maybe it's time to finally pay attention to this open-source stuff. You look around to survey the damage one more time. Besides, ditching the Oracle license might just about pay for this.</p>

<p>"Yeah," you reply, "I'll be there in five minutes. Close the door, please."</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open-Source GIS Bootcamp at Salisbury University]]></title>
    <link href="http://blog.geomusings.com/2013/05/23/open-source-gis-bootcamp-at-salisbury-university/"/>
    <updated>2013-05-23T05:33:00-04:00</updated>
    <id>http://blog.geomusings.com/2013/05/23/open-source-gis-bootcamp-at-salisbury-university</id>
    <content type="html"><![CDATA[<p><a href="http://www.linkedin.com/groupItem?view=&amp;srchtype=discussedNews&amp;gid=3300945&amp;item=242507385&amp;type=member&amp;trk=eml-anet_dig-b-pop_ttl-hdp&amp;ut=12tcrQvogVeRM1">Thanks to LinkedIn</a>, I saw that Dr. Art Lembo of Salisbury (Maryland) University is leading an "Open Source/Enterprise GIS Summer Bootcamp" at the university from June 3 - 7, 2013. All of the salient details, including contact information, <a href="http://www.esrgc.org/bootcamp/SUBootCamp.pdf">can be found here</a> (PDF).</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/river.jpg" /></p>

<p>Having seen Dr. Lembo and his team in action <a href="http://blog.geomusings.com/2013/03/20/the-best-thing-i-saw-at-tugis-2013/">for an afternoon at TUGIS</a>, I think this will be a good way for those who have been wanting to take the leap with open-source GIS tools to get some hands-on experience with core tools like <a href="http://qgis.org/">QGIS</a> and <a href="http://postgis.net/">PostGIS</a>. It's also a great time of year to be on Maryland's Eastern Shore. The LinkedIn discussion says there are still spaces available but the date is coming up soon so you'll want to move quickly if you're interested.</p>
]]></content>
  </entry>
  
</feed>
