<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[geoMusings]]></title>
  <link href="http://blog.geomusings.com/atom.xml" rel="self"/>
  <link href="http://blog.geomusings.com/"/>
  <updated>2014-10-08T23:55:40-04:00</updated>
  <id>http://blog.geomusings.com/</id>
  <author>
    <name><![CDATA[Bill Dollins]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[testing]]></title>
    <link href="http://blog.geomusings.com/2014/10/08/testing/"/>
    <updated>2014-10-08T22:04:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/10/08/testing</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Personal Thoughts On the AppGeo Announcement]]></title>
    <link href="http://blog.geomusings.com/2014/09/11/personal-thoughts-on-the-appgeo-announcement/"/>
    <updated>2014-09-11T20:06:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/09/11/personal-thoughts-on-the-appgeo-announcement</id>
    <content type="html"><![CDATA[<p>I read with great interest <a href="http://www.appgeo.com/blog/breaking-up-is-hard-to-do/">today&#8217;s announcement that AppGeo is no longer an Esri Business Partner</a>. I find the announcement significant for a number of reasons, which I will explore shortly. I have always respected <a href="http://www.appgeo.com/">AppGeo&#8217;s</a> work. As a small business that does geospatial consulting, they have foregone the &#8220;grow at all costs&#8221; approach that is seen all too often in the consulting world. They generally stuck to what they do well and branched out conservatively in ways that tie logically back to their core business.</p>

<p>I first met the President of AppGeo, Rich Grady, at an early <a href="https://www.hifldwg.org/">HIFLD</a> meeting many years ago. (It may have even been before the group was called &#8220;HIFLD.&#8221;) The work they were doing then was very relevant to critical infrastructure protection efforts and, had some of their concepts for sharing data between state, local, and Federal agencies been adopted, we&#8217;d probably be better off today. I have always considered Rich one of the good guys in the geospatial industry and the company he has built reflects his integrity.</p>

<p>Over the years, our companies haven&#8217;t quite found the right vehicle to work together and we sometimes even compete against each other. That&#8217;s the nature of the consulting business. You will often compete against friends and still be able to have dinner together later.</p>

<p>So I was happy for Rich and AppGeo when I read their announcement. As I said above, I found it significant in a few ways&#8230;</p>

<!--more-->


<h2>Professionalism</h2>

<p>The announcement was a textbook example of the professionalism with which such matters should be handled. AppGeo did not burn down the house in announcing they were no longer a business partner. There were no declarations that &#8220;Esri sucks&#8221; or any other such unnecessary hyperbole. They discussed real, significant differences that had evolved over time with Esri that lead to an agreement that their business interests had diverged to the point that a Business Partner status no longer made sense. While Esri initiated the final break, AppGeo handled it well, stated their case professionally, and gave no ground. Well done. This announcement should be required reading for the various factions and camps in our industry.</p>

<h2>A Sign of Maturity</h2>

<p>I found the announcement significant because I see it as evidence that AppGeo, as a geospatial business, has evolved to a point of maturity that most businesses never reach. They have built a diverse portfolio and realize they can go it alone without the safety net of Redlands under them. I will note that my company is an Esri Business Partner so I guess we haven&#8217;t gotten there yet. I have worked as a consultant for my entire career and I believe to my bones that my primary job is to recommend the most appropriate solution to my customers. That is quite often an Esri solution for a lot of valid reasons, but not always. Right now, I am working on a project for a Federal agency based entirely on open-source geospatial tools. Often, a hybrid solution is really the best fit. The point is that, as a consultant, my job is to first gather data, analyze it, and determine the best solution. My job is not to walk in with a pre-conceived notion. If you call yourself a consultant and never look outside the Esri stable of tools (or whomever your preferred vendor may be), then you are not a consultant but a salesman. Just own it.</p>

<p>AppGeo recognizes this and has built a consulting practice that can proceed forward with less influence from large vendors. I commend them for that.</p>

<h2>Maturity of Open-Source Geospatial</h2>

<p>This next observation is statistically spurious as it is based only on this particular case, but I see this announcement as a sign of the increased maturity of the open-source geospatial segment. AppGeo is still a fairly small business, and many small consulting shops tend to be very conservative in their business moves. The fact that they are willing to decouple themselves from Esri indicates that they are not only confident in their capabilities with a diverse tool set, but that they are also confident in the stability and reliability of the tools in terms of being able to stake their business on them. Those of us who work with open-source geospatial tools are not surprised by this but it is important to note that AppGeo is as deeply proficient with the Esri tool suite as any company I&#8217;ve seen. I am not privy to their internal decision-making process but, at some point, a comparison must have yielded the conclusion that open-source tools (and Google) were sufficient to stand on equal footing in their capability portfolio. When a once-marquee partner like AppGeo reaches that conclusion, I find it significant.</p>

<p>Those represent some of my thoughts after reading the announcement. I recognized a lot in it but I can&#8217;t say I&#8217;ve had all of the experiences they describe. I can&#8217;t say that I&#8217;ve ever received any poor treatment from Esri as a Business Partner. I have heard enough stories from others that I don&#8217;t doubt that it happens but I have never personally experienced it. Esri has always been exceedingly professional with me and my company. I don&#8217;t feel like I&#8217;ve ever felt undue pressure to push their products to the front of the line but I recognize that my experience may be atypical.</p>

<p>In many ways, I expect all vendors to be partisan about their products, be it Esri or Microsoft or Oracle or Boundless or whomever. I would be suspicious if they weren&#8217;t. Perhaps, because of that expectation, I&#8217;ve developed a bit of a filter for it but it doesn&#8217;t bother me. My job as a consultant is to stand between my customers and the vendors and use my experience to sort through the market blather, find the real data, help my customers make an informed decision, and then help them implement. I have always felt that proficiency with a broad set of technologies makes me a better consultant, even for my strictly-Esri customers. AppGeo clearly understands this better than most and I commend them on reaching this significant and positive milestone.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Maryland Council on Open Data]]></title>
    <link href="http://blog.geomusings.com/2014/08/26/maryland-council-on-open-data/"/>
    <updated>2014-08-26T17:25:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/08/26/maryland-council-on-open-data</id>
    <content type="html"><![CDATA[<p>Back in May, I had the honor of being appointed to the newly established Maryland Council on Open Data. The Council had its inaugural meeting in Baltimore yesterday and was heavily attended, including attendance by Governor Martin O&#8217;Malley. I&#8217;ll discuss his remarks to the group later.</p>

<p>As the first meeting of a new group, it went off largely as I expected. The agenda consisted primarily of an overview of the establishing legislation, a review of ethics requirements, demos of the existing open data portals, discussion of the history of open data in Maryland, and remarks from the Governor.</p>

<p>I won&#8217;t go into details about the make-up of the Council, but they can be found here <a href="http://msa.maryland.gov/msa/mdmanual/25ind/html/53opendata.html">http://msa.maryland.gov/msa/mdmanual/25ind/html/53opendata.html</a>. Nor will I do a deep dive into the legislation, but it can be found here: <a href="http://mgaleg.maryland.gov/2014RS/chapters_noln/Ch_69_sb0644T.pdf">http://mgaleg.maryland.gov/2014RS/chapters_noln/Ch_69_sb0644T.pdf</a> (PDF). I will instead focus on my take-aways from the meeting itself.</p>

<!--more-->


<p>First, the establishing legislation makes data in Maryland open by default, unless it falls under certain criteria (personally identifiable information, law enforcement sensitive, etc.). Second, it is in fact legislation. Previous open data initiatives (MSGIC Executive Committee and the Maryland Open Data Working Group), were established by executive order. As a result, they were vulnerable to reversal by subsequent administrations and they had no real effect on other branches of government. Because of the new legislation, the open data has greater durability and active, enthusiastic participation from the state legislature.</p>

<p>So the new Council unifies the previous efforts and has top cover from the legislation. That can only be a good thing. The state currently has two open data portals: an <a href="http://data.imap.maryland.gov/">Esri Open Data portal</a> for geospatial data and a <a href="https://data.maryland.gov/">Socrata portal</a> for everything else. In practice, the lines between them may not be so distinct, but that&#8217;s the stated role of each.</p>

<p>It is clear that open data is important to the Governor. Since his days as the Mayor of Baltimore, he has been known as a data-driven executive. The &#8220;CityStat&#8221; concept in Baltimore evolved into &#8220;StateStat&#8221; when he moved to Annapolis. It is widely known that data and metrics back everything his administration does. Open data is the other side of the coin. It is the mechanism by which the supporting data and metrics of the state government are made public.</p>

<p>In his remarks, the Governor highlighted Maryland&#8217;s top ranking (shared by six states) in the <a href="http://www.datainnovation.org/2014/08/state-open-data-policies-and-portals/">Center for Data Innovation report of August 18, 2014</a> but then quickly addressed the remaining weaknesses identified. Specifically, he discussed:</p>

<ul>
<li>Reporting on spending data</li>
<li>The need for a more complete picture of all data sets</li>
<li>The need for better minimum metadata standards</li>
<li>The need for a uniform standard for giving citizens access to public information</li>
</ul>


<p>These are all fairly easy to address on the surface. In the open discussion that followed, the theme that I took away was &#8220;culture.&#8221; Maryland has the policy framework and a good start on a technical framework to more fully open its data. The hard work, as it always is, is transforming the culture. As the requirements for opening data begin to trickle down into the daily lives of the people handling information, I think a lot of workflows will change. I also suspect some of the standards chosen for achieving compliance with open data requirements will facilitate significant changes to technical architectures in individual departments. In short, the proverbial onion will part to get peeled.</p>

<p>It&#8217;s a lot of work and it won&#8217;t happen quickly. I am excited for the opportunity to participate. I have worked in the private sector for my entire career (although in professional services to the government), so this is my first foray into anything resembling being on the public sector side of things. I expect I&#8217;ll have a lot to learn.</p>

<p>My interest in open data has been somewhat spurred by the infrastructure data contraction that occurred in the mid-2000s in reaction to domestic security concerns. I felt it was the wrong direction to go then and I don&#8217;t think making data harder to acquire really helped anyone. I&#8217;m looking forward to seeing what we can do on the Council to achieve the benefits other states, like Arkansas, have achieved through opening data rather than closing it off.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JS.GEO 2014 Locked In Solid]]></title>
    <link href="http://blog.geomusings.com/2014/07/31/js-dot-geo-2014-locked-in-solid/"/>
    <updated>2014-07-31T16:32:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/31/js-dot-geo-2014-locked-in-solid</id>
    <content type="html"><![CDATA[<p>A while back, <a href="http://blog.geomusings.com/2013/12/27/js-dot-geo-2014-announced/">I posted about about the 2014 edition of JS.GEO</a>. After that post, things got a little fluid, but I&#8217;m happy to finally be able to provide an update.</p>

<p>According to JS.GEO Organizer <a href="http://twitter.com/cwhelm">Chris Helm</a>, the event is <a href="https://twitter.com/cwhelm/status/494919297144066048">&#8220;locked in solid.&#8221;</a></p>

<p>The event has a <a href="http://jsgeo.com">web site</a>, a location, and a date. <a href="http://www.eventbrite.com/e/jsgeo-2014-tickets-12450450633">Tickets and sponsorships are available.</a> Agenda is to be announced and they are actively seeking speakers.</p>

<p>The first event was one of those serendipitous things that turned out to be pretty awesome. It was personally very influential on me and changed the focus of a lot of what I was doing with geospatial and with programming. It&#8217;s probably the best single geospatial event I&#8217;ve attended in the last five years. The fact that FOSS4G will have already drawn a like-minded crowd to Portland should bode well for JS.GEO. My own attendance is still in question due to a number of factors but I highly recommend adding this to your schedule if you can.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ArcWhat? I Just Want My Map.]]></title>
    <link href="http://blog.geomusings.com/2014/07/30/arcwhat-i-just-want-my-map/"/>
    <updated>2014-07-30T15:19:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/30/arcwhat-i-just-want-my-map</id>
    <content type="html"><![CDATA[<h3>TL;DR:</h3>

<p>What follows is probably my last post related to the Esri User Conference and is highly Esri-centric. Open-source readers may want to jump off here, or exercise a willing suspension of disbelief.</p>

<hr />

<p><a href="http://blog.geomusings.com/2014/07/16/the-esri-uc-so-far/">A couple of posts ago</a>, I did something that I generally try to avoid. I took <a href="http://www.esri.com">Esri</a> to task for its confusing product names without really offering any thoughts on how to make things better. I don&#8217;t really like it when people do that to me so I&#8217;ll try to correct that here. It bears noting that I was not the only person feeling this way at the UC. I was happy to see <a href="http://www.directionsmag.com/articles/making-sense-of-the-2014-esri-international-user-conference/410201">Adena&#8217;s post over at Directions</a> touch on this and it also came up in a number of conversations I had while I was in San Diego.</p>

<p>Here are some things that I think may help. They represent most of the stumbling blocks I typically encounter when doing consulting/integration with Esri-centric users, especially new ones.</p>

<p style="text-align:center;"> <img src="http://upload.wikimedia.org/wikipedia/commons/9/93/Spaghetti.jpg" /> <br/>
 <sub>&#8220;<a href="http://commons.wikimedia.org/wiki/File:Spaghetti.jpg#mediaviewer/File:Spaghetti.jpg">Spaghetti</a>&#8221;. Licensed under <a href="http://creativecommons.org/licenses/by-sa/3.0/" title="Creative Commons Attribution-Share Alike 3.0<p></p>&#8220;>CC BY-SA 3.0</a> via <a href="http://blog.geomusings.com//commons.wikimedia.org/wiki/">Wikimedia Commons</a>.<sub></p>

<!--more-->


<ol>
<li><p>Retire the &#8220;Arc&#8221; prefix - It&#8217;s confusing to have a relatively meaningless and seemingly random (to new users) prefix attached to everying. If you are moving to a platform concept, shouldn&#8217;t that platform just bear your name? The company is Esri, the platform should be &#8220;Esri GIS&#8221; or something. Don&#8217;t hide from your own product line. &#8220;Arc&#8221; is a verbal stumbling block. Try telling a user how to publish a map online and see how many times you say &#8220;Arc.&#8221; Is it any wonder they get confused? It&#8217;s like the old dog food commercial, except all they hear is &#8220;Blah blah Arc blah Arc blah blah Arc.&#8221;</p></li>
<li><p>Acknowledge four environments for the platform - Mobile, Desktop, Server, Online. Period. &#8220;Esri GIS Desktop&#8221; or &#8220;Esri GIS Server&#8221; for example. Clean up spurious products that just clutter the menu. The current Explorer? Esri GIS Mobile (resist the urge to tack on &#8220;for iOS&#8221;). The current Portal? Esri GIS Online (Online can offer a hosted or on-premise version). What&#8217;s the distinction between Esri GIS Server and an on-premise instance of Esri GIS Online? I don&#8217;t know. Those are your waters to unmuddy.</p></li>
<li><p>One Desktop - Basic? Standard? Advanced? How about just Desktop? After 15 or so years of selling the three tiers, you should have enough data on the sales mix to set one normalized price for a fully-functional desktop product and be done with it. I suspect that price would hover somewhere between the current prices of Basic and Standard, perhaps closer to Standard. Everybody pays one price, everybody gets the current Advanced product. If nothing else, that code base should have long since paid for itself. Then you&#8217;ll be better positioned for the inevitable transition to Esri GIS Pro (which could really just be the next Esri GIS Desktop) that everyone sees coming. Need more? Buy extensions.</p></li>
<li><p>Speaking of Extensions - You can have too much of a good thing. Roll some of them into core products (like Spatial Analyst). Doing so will assure users that a common set of tools will be available as they move between the environments (see item number 2). Furthermore, stop calling everything an &#8220;extension,&#8221; which exacerbates the need for cumbersome qualifiers like &#8220;for Desktop&#8221; and &#8220;for Server.&#8221; Choose a standard nomenclature for each environment and stick with it. Desktop can have &#8220;extensions&#8221; and server can have &#8220;adapters,&#8221; for example. Then I know that a discussion about the Network Analyst Adapter is focused on Server.</p></li>
<li><p>Stop changing product names - Take any of the suggestions above, or do something else entirely. But, whatever you do, leave it alone once you&#8217;re done. At least for a couple of dot releases, anyway.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slow Food]]></title>
    <link href="http://blog.geomusings.com/2014/07/29/slow-food/"/>
    <updated>2014-07-29T09:21:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/29/slow-food</id>
    <content type="html"><![CDATA[<p>In 1985, I was a junior in high school and I got my first job at a local chain steakhouse. I ended up staying there for a few years and did everything, including management. This particular location happened to be the busiest store in the chain, which had a couple hundred locations at the time. Basically, we just unlocked the doors and people came in. We often had a line and managers from all over the country came to see how we did business.</p>

<p style="text-align:center;"> <img src="http://s3-media3.fl.yelpcdn.com/bphoto/NhIU66uSYbsi9LPzBpSgwA/l.jpg" /></p>

<p>Eventually, I transferred to another store that happened to be much slower. I expected this to be a something of a cakewalk compared to the store I had just left. Sometime during my first day, the long-time manager made a point to remind me to close the back dining room after lunch and turn the lights out. The dark room looked uninviting to me so I asked if we could leave the lights on. He replied that doing so would raise the electric bill and affect the store&#8217;s profits. Over the next few weeks, I learned so many new techniques for managing food inventory, staffing levels, and equipment that I realized my initial impression was wrong. So I told the manager this. He was not surprised.</p>

<p>He said to me: &#8220;It takes more skill to run a slow store than a busy one.&#8221;</p>

<!--more-->


<p>He was absolutely right. At the first location, we would simply order enough food to fill the walk-ins and did enough prep every day to fill the racks. We quickly ramped our staffing up to max levels every day and just waited for the customers to roll in. There was a myriad of issues we simply never had to worry about because sales volume masked them.</p>

<p>It&#8217;s been many years since I&#8217;ve had to work in a restaurant but I have found this observation applicable throughout my career, during which I have worked primarily in the defense industry, designing and building various forms of geospatial tools and systems. Two weeks ago, I attended the Esri International User Conference. The Esri UC is full of opportunities to see more of these types of applications, from the Defense Showcase to the National Security Summit to many defense/intel/homeland security related paper sessions.</p>

<p>I attended almost none of them.</p>

<p>Over the past few years, I have developed the habit of attending sessions related to local governments, non-profits and other small organizations. These organizations tend to have budgets that are essentially rounding error in the defense world and I have become much more intrigued by the solutions they build to meet their missions with so few resources.</p>

<p>Of course, I was at an Esri conference, so everyone I saw was an Esri user. It&#8217;s no secret that Esri tools tend to be expensive and every dollar counts for small governments, so I am intrigued how these organizations arrive at the choice of Esri tools in light of so many other capable options. Too often, there is a misconception that these small governments simply choose Esri-based solutions out of a lack of understanding of alternatives. What I have discovered is, like the manager of the slow restaurant, some of the best business understanding and most cogent and well-reasoned justifications come from these small users.</p>

<p>They seem to take a much more holistic approach to evaluating their solutions; including the availability of support from the technology provider as well as the existence of a robust third-party community that enables them to effectively compete for value-added services, customizations, and consulting. What I found interesting has been the most consistent attraction of local governments and small organizations to Esri-based solutions: the seamless integration of the entire ArcGIS platform. From mobile to desktop to web, they find that Esri tools make it easier to move through the product lifecycle. This exposes a very business-centric way of thinking that is larger than any individual technology or its licensing cost.</p>

<p>As I indicated above, an Esri conference places a huge filter on the users whom you meet. As I attend less vendor-centric events in the months ahead, I&#8217;d like to continue this line of inquiry. While landing the giant white whales of large Federal agencies might be more satisfying for the short-term bottom line, it is important to remember that all levels of government are facing downward budget pressure. Learning the thought processes of organizations that have always had to tightly manage limited resources may be more valuable in the long run.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock-In]]></title>
    <link href="http://blog.geomusings.com/2014/07/24/lock-in/"/>
    <updated>2014-07-24T09:13:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/24/lock-in</id>
    <content type="html"><![CDATA[<p>I&#8217;ve been a consultant/programmer/integrator/other for over twenty years now. That&#8217;s not quite long enough to say I&#8217;ve seen it all but long enough to notice a few patterns. Admittedly, I&#8217;ve spent the vast majority of that time working in the defense world so the patterns may be heavily skewed to that but I think not.</p>

<p style="text-align:center;"> <img src="http://upload.wikimedia.org/wikipedia/commons/9/93/US_Navy_080904-N-9079D-037_Operations_Specialist_3rd_Class_Sarah_M._Hernandez_stands_the_global_command_and_control_systems-maritime_watch.jpg" /></p>

<p>I&#8217;ve run across a number of well-entrenched government-developed systems, such as command-and-control systems, with user interfaces and experiences that many professional designers would consider abhorrent. Yet, I have watched smart, motivated men and women in uniform stand in front of working groups and committees dedicated to &#8220;improving&#8221; systems and workflows and advocate passionately for these seemingly clunky systems.</p>

<p>Why? Because they know how to use these systems inside and out to meet their missions. User experience is ultimately about comfort and confidence. A user that is comfortable with a system will have a great experience with it regardless of its appearance. DOD tackles this reality through training. For all its faults, there is still no organization better at developing procedures and thoroughly training its people in them. It results in a passionate loyalty for the tools that help them do their jobs and places a very high hurdle in front of any tools that seek to replace current ones.</p>

<!--more-->


<p>This experience has given me a different view of the concept of &#8220;lock-in.&#8221; Over my career, I have heard this term used in a pejorative sense, usually proceeded by the word &#8220;vendor.&#8221; Although I have used the term myself, I usually hear it levelled by a vendor&#8217;s competitors. It is typically meant to refer to practices a vendor uses to establish barriers to exit for its customers, making it harder for them to choose a competing technology. Such practices can include artificial bundling of unrelated tools, license trickery, half-truths in marketing, and many more; all of which do happen.</p>

<p>Lock-in is a real thing. Lock-in can also be a responsible thing. The organizations I have worked with that make the most effective use of their technology choices are the ones that jump in with both feet and never look back. They develop workflows around their systems; they develop customizations and automation tools to streamline repetitive tasks and embed these in their technology platforms; they send their staff to beginning and advanced training from the vendor; and they document their custom tools well and train their staff on them as well. In short, they lock themselves in.</p>

<p>This is the right and responsible thing to do. An organization, once it has selected a technology, has a responsibility to master it and use it as effectively as it can. If you start applying numbers to all the activities listed above, you will quickly see that it is an investment that far outstrips the original investment in the technology itself. In fact, the cost of the technology itself is often seen as marginal to the overall lifecycle cost, which makes arguments about removing licensing costs, for example, less effective than they would appear to be.</p>

<p>This is true regardless of the provenance of the technology. The original technology has to start to become a hindrance before change is seriously considered, which I am seeing in a few cases these days. But, by and large, the very strong pattern I have seen is that the majority of lock-in originates with users. To fail to recognize that and continue to target the vendor is to miss the point and, ultimately, the target.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Esri UC So Far #EsriUC]]></title>
    <link href="http://blog.geomusings.com/2014/07/16/the-esri-uc-so-far/"/>
    <updated>2014-07-16T12:13:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/16/the-esri-uc-so-far</id>
    <content type="html"><![CDATA[<p>So I&#8217;m halfway through the largest geospatial event of the year, attending it for the first time in four years, and I haven&#8217;t blogged yet. As always, it&#8217;s a busy week. Because this event draws people from all over the country (and world), my dance card fills up pretty quickly. And, by the way, there&#8217;s a conference going on.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/plenary_stage.jpg" /></p>

<p>This is the first I&#8217;ve ever attended the Esri User Conference as just an attendee. If it were a video game, I&#8217;d be playing it on the easy level. I sat through the entire plenary for the first time in years. It was nice table setting for the rest of the week. As the father of a dancer, I have developed an eye for choreography and there is plenty of it up on the plenary stage. If I were to level one piece of constructive criticism toward the UC, it&#8217;s that I&#8217;d let speakers be themselves a little bit more. That said, the content was delivered smoothly, which is really the larger point.</p>

<!--more-->


<p>The plenary features a lot of demos that are rehearsed to within an inch of their lives. Knowing that, I still found ArcGIS Pro to be interesting. The UI is well-designed to get out the user&#8217;s way and it&#8217;s native 64-bit architecture finally allows it to take advantage of system resources in a way that ArcMap never could. The UI is more modern, featuring the ribbon toolbar. Esri seems to have learned a lesson from Microsoft by not re-engineering the UI of a familiar product, but releasing a new product that users can transition to. That was smart. In Microsoft Office terms, the UI upates from the Office 97 feel of ArcMap to more of an Office 2012 feel. Tasks in ArcGIS simplify geoprocessing even more but also run the risk of hiding complexity too much. I hope we&#8217;re not training another generation of button-clickers.</p>

<p>One of the more subtle announcements in the plenary was that ArcGIS Portal will be included with ArcGIS Server at 10.3. I suspect that&#8217;s the beginning of a gentle nudge of Server into ArcIMS-like oblivion but that&#8217;s just my speculation.</p>

<p>The newly-released (not beta) ArcGIS Open Data extension (product?) was shown by <a href="http://twitter.com/ajturner">Andrew Turner</a>. It does a very nice job of putting an almost-GeoCommons-easy interface in front of ArcGIS Online to enable organizations to easily share their data. Andrew showed the implementations for the DC government and the State of Maryland. Maryland, in particular, is exceedingly happy with how quickly it has helped them start sharing data. By chance, I had dinner with some of the development team the night before and it&#8217;s a motivated team with a diverse skill set not rooted in traditional Esri thinking which, in my opinion, bodes well for what they are doing.</p>

<p>Yesterday, I sat through a couple of sessions on the GeoEvent Processor (GEP), which has already been rebranded as the GeoEvent Extension for Server (or something like that). Those who have tracked my blog over the years know that I&#8217;ve done a lot of work with situational awareness systems. Within that field, I&#8217;ve concentrated a lot on feeds (streaming data sources) and track management so it&#8217;s something of a geeky passion for me. Esri&#8217;s previous, long-standing, and wholly awful product in this space was Tracking Server. I would gleefully dance around Tracking Server&#8217;s funeral pyre.</p>

<p>GEP seems to be a better attempt at solving this problem. In its current incarnation, it simply doesn&#8217;t have the throughput my customers will need (it currently supports about 800 - 1000 messages per second). To the product team&#8217;s credit, they know this and are focusing on it. At 10.3, GEP will support &#8220;clustering&#8221; to increase throughtput. Clustering, however, was not explained during the session and could mean anything so I&#8217;ll wait and see. The management interfaces and APIs behind GEP are top-notch and enable some of the use cases I&#8217;ve supported in the past. Once the throughput challenges are solved, this could be a product to watch.</p>

<p>You may have noticed a few parenthetical comments above when referring to product names. The reason for that is that Esri&#8217;s product naming has fallen off the cliff to become just confusing, unwieldy, and random. Couple that with the fact that product names change constantly, or are recycled endlessly (Explorer?), and it&#8217;s a complete mess. It&#8217;s always been difficult walking new users through the maze of Esri products and the current naming &#8220;convention&#8221; doesn&#8217;t help at all. Esri really needs to circle the wagons on this and simplify things.</p>

<p>That sums up my week so far. I&#8217;ll try to check in again before I go.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gearing Up For the Esri UC]]></title>
    <link href="http://blog.geomusings.com/2014/07/09/gearing-up-for-the-esri-uc/"/>
    <updated>2014-07-09T16:05:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/09/gearing-up-for-the-esri-uc</id>
    <content type="html"><![CDATA[<p>With a house move behind us and a lot of unpacking and other tasks ahead, I am nonetheless getting ready to head out to the <a href="http://www.esri.com/events/user-conference">Esri International User Conference</a> next week. This will be my first time attending since 2010 and is the first UC since then that has aligned with my schedule in a way that I can make it. Of course, the price is right this year as well ($0.00).</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/office_view.png" /></p>

<p>The &#8220;big&#8221; UC has steadily dropped in significance for me over years as it has become much easier to get Esri-related information through various other channels; primarily through social media and local/regional events such as the <a href="http://www.esri.com/events/federal">FedUC</a> and local dev meetups. The last few trips to San Diego left me feeling that the content presented there is getting increasingly superficial compared to the other events. This year, however, I have been in the midst of building a house and moving so I have not had the time to attend the smaller events. As a result the UC makes sense. I am hoping the recent trend reverses itself.</p>

<p>I still do some Esri-based consulting so it&#8217;s important to stay current however I can. My government customers are starting to at least ask about ArcGIS Online, so I want to finally get my mind around it as best I can. The messaging around that platform has been so muddled that it&#8217;s still difficult for me to articulate what productivity advantages, if any, it actually offers. My own experimentation with it has left me wanting. The fact that it has been <a href="http://www.executivegov.com/2014/06/agriculture-dept-grants-fisma-certification-to-esri-cloud-mapping-tech/">certified as FISMA compliant</a> will certainly raise its profile with some of my Federal customers, though that&#8217;s typically only the first step in a very long process. I&#8217;m also curious about ArcGIS Pro.</p>

<p>Unlike previous years, I won&#8217;t be manning a booth (we sold ours a few years ago) or directly representing a customer so I&#8217;ll actually have the luxury to attend sessions and meet up with people. There are some people who, unfortunately, I really only see face-to-face at such large magnet events so I&#8217;m looking forward to catching up with them, as well as meeting new people. I will, however, be popping into the paper session of one of my co-workers. (It would be awesome if the online agenda provided permalinks to individual sessions.)</p>

<p>So, if you&#8217;re going to be in at the UC next week and would like to meet up, feel free to ping me on Twitter (<a href="https://twitter.com/billdollins">@billdollins</a>), via e-mail (bill [at] geomusings [dot] com), or drop a comment below.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CFPB Fellowship Seeking 2015 Candidates]]></title>
    <link href="http://blog.geomusings.com/2014/07/07/cfpb-fellowship-seeking-2015-candidates/"/>
    <updated>2014-07-07T13:48:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/07/07/cfpb-fellowship-seeking-2015-candidates</id>
    <content type="html"><![CDATA[<p>It&#8217;s no secret that I am contractor who spends a lot of my time attempting to develop software for defense users. I&#8217;ve been doing this for a long time, though I have added other customers to my portfolio over the years. The process of development in this arena gets more frustrating by the day. Recently, for example, a group policy update was pushed that removed any browser other than Internet Explorer from our development machines and rolled Internet Explorer back to version 9. These are just the latest such setbacks to productivity and they represent every stereotype we&#8217;ve ever heard about Federal Government computing.</p>

<p>Thankfully, there are countervailing examples which point to how things could be. One such example is the <a href="http://www.consumerfinance.gov/jobs/technology-innovation-fellows/">Technology and Innovation Fellowship at the Consumer Financial Protection Bureau (CFPB)</a>. This two-year fellowship program provides an opportunity to show how Federal software can be developed, with an <a href="https://github.com/cfpb/source-code-policy/blob/gh-pages/cfpb-source-code-policy.txt">open-source-first approach</a>, and also how software development can occur, via remote teams and distributed collaboration. These are not new concepts in the overall marketplace but are still fairly exotic in the Beltway region. Ultimately, the fellowship holds out the possibility of building technology that actually helps government work better and shows how modern tools and working arrangements be applied in the Federal Government.</p>

<p>I was clued into this fellowship program via a tweet by <a href="https://twitter.com/byrne_tweets">Mike Byrne</a>, who has already helped show the way via the <a href="http://www.broadbandmap.gov/">National Broadband Map</a> at the FCC, and who is now at the CFPB. There are very few people I&#8217;ve met in the Federal Government who have a better vision for modernizing IT development and acquisition, coupled with the ability to get things done. If you&#8217;re of a technical bent and looking to work inside the Federal Government, this fellowship program may be something you want to check out.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where Ya Been?]]></title>
    <link href="http://blog.geomusings.com/2014/06/20/where-ya-been/"/>
    <updated>2014-06-20T10:14:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/06/20/where-ya-been</id>
    <content type="html"><![CDATA[<p>It&#8217;s been rather quiet on the blog for a while. Sometimes the posts have to take a back seat to work and other things. This time of year tends to be busy anyway due to the end of the school year and its related activities, but this year has also included one move, construction of a house, and preparations for a second (final) move. In December we sold our house, which I had lived in for nearly 40 years, and moved into temporary quarters while the next house was being built. The sale of the old place was a pretty smooth experience as all of us, especially me, were ready for a change.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/sunrise_crop_small.jpg" /></p>

<p>As a result, the experimentation and small projects which have driven the content of this blog since it started simply had to stop for a while. That&#8217;s not to say that there has been no activity. I have posted over the last few months related to some mapping work and the &#8220;software exhaust&#8221; that has resulted from it. It&#8217;s not really been possible, however, to sit down a create a well-structured discussion of those activies in the way that I would prefer, so I simply haven&#8217;t.</p>

<!--more-->


<p>Because that work involved the use of <a href="https://www.mapbox.com/foundations/an-open-platform/#mbtiles">MBTiles</a>, it got me a little closer to some of the open-source tools produced by <a href="http://mapbox.com">MapBox</a> in a way that I haven&#8217;t really needed to before. In addition to <a href="https://github.com/mapbox/mbutil">MBUtil</a> and <a href="https://www.mapbox.com/tilemill/">TileMill</a>, I&#8217;ve been able to use some of their <a href="http://leafletjs.com/">Leaflet</a> Javascript plug-ins for use in data visualiztion. My overall observation is that I&#8217;ve been very impressed with the quality of the tools they are putting out in terms of performance, stability, and elegance. It&#8217;s been a pleasant experience working with their tools and I&#8217;ve also learned a lot by digging into their source.</p>

<p>It reminds me a lot of the experience I had a few years ago working with the GeoIQ tool set. That tool set resonated with me for the same reasons that the MapBox tool set does now; I feel like the authors of the tools think about problems and approach them in a manner similar to the way I do. As a result, I find the tools comfortable and not something I feel like I am fighting with. I&#8217;m not certain that this psychological aspect of software is given much attention but software is really a representation of the author&#8217;s approach to problem-solving. For me, developing proficiency with a piece of software is establishing a connection with its author in a manner similar to that which is established with the author of a good book. It&#8217;s one of the reasons I consider releasing open-source code to be such a brave thing.</p>

<p>It&#8217;s my hope that I&#8217;ll find ways to keep working with the MapBox tool set more, but such considerations are always driven by project availability and customer demand. In the meantime, there&#8217;s a move to complete, a new house to settle into, and a summer to enjoy.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GovWin Analyzes Federal Spending in the DC Area]]></title>
    <link href="http://blog.geomusings.com/2014/05/26/govwin-analyzes-federal-spending-in-the-dc-area/"/>
    <updated>2014-05-26T15:16:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/05/26/govwin-analyzes-federal-spending-in-the-dc-area</id>
    <content type="html"><![CDATA[<p><a href="http://www.deltek.com/products/govwin">GovWin</a>, the procurement research service of <a href="http://www.deltek.com/">DelTek, Inc</a>. recently released a report titled <a href="http://more.deltek.com/LP=4108">&#8220;Federal Prime Spending in DC, MD, and VA&#8221;</a> (PDF, registration required) which analyzed federal spending patterns in those areas. The report included the top 20 counties, by total spending from FY10 to FY14. With permission from the GovWin research team, I made a simple interactive map of the data.</p>

<iframe src="http://blog.geomusings.com/assets/demos/govwin/map.html" marginwidth="0" marginheight="0" scrolling="no" style="width: 672px;height: 350px;border: 1px solid #DEDEDE"></iframe>




<!--more-->


<p>The jurisdictions highlighted here are not particularly surprising, though the exclusion of Charles County, Maryland and King George County, Virginia caught my eye. Perhaps the inclusion of the Hampton Roads, Virginia rather than limiting the study to the usual Beltway region caused them to drop out. Those counties are home to Navy bases that provide a significant amount of jobs but the presence of a single base is clearly not enough to boost a county in comparison to those with a more diveresified presence of federal spending.</p>

<p>In some ways, the data follows the adage that all maps really just show population. In this case, the spending patterns generally follow the &#8220;dumbbell&#8221; (the Baltimore and Capital beltways connected by I-95), extended to include the I-66 corridor. The jurisdictions in this area feature large populations, significant presence of Federal agencies, and numerous offices of supporting contractors. The same holds true for the Hampton Roads area. The only real exception to this pattern is St. Mary&#8217;s County, Maryland. Still largely rural, it benefits from the presence, in addition to a Naval Air Station, of the headquarters of Naval Air Systems Command; giving a boost from a significant amount of procurement related to Naval air power.</p>

<p>When charting out the year-by-year data, the general trend of decreasing government spending becomes apparent. This may not be a bad thing, as it is causing some local jurisdictions to look at strategies for diversifying away from heavy reliance on federal spending, especially from a single source.</p>

<h2>Top 20 Counties Ordered by Total Procurement <br/></h2>

<iframe src="http://blog.geomusings.com/assets/demos/govwin/chart.html" marginwidth="0" marginheight="0" scrolling="no" style="width: 672px;height: 510px;border: 1px solid #DEDEDE"></iframe>


<p>What I found interesting was when I added the American Community Survey data for 2012 into the mix. By calculating the per capita procurement spending using the 2012 population estimate, the ordering of the jurisdictions changed noticeably. The most dramatic was Mannassas City, VA which is last in total procurement spending but fourth in per capita spending. Similarly, St. Mary&#8217;s County, Maryland jumps from 13th to 5th when looking at per-capita spending. Conversely, Baltimore County, Maryland drops to 19th despite being 6th in total spending.</p>

<h2>Top 20 Counties Ordered by Per Capita Procurement <br/></h2>

<iframe src="http://blog.geomusings.com/assets/demos/govwin/percapita.html" marginwidth="0" marginheight="0" scrolling="no" style="width: 672px;height: 510px;border: 1px solid #DEDEDE"></iframe>


<p>There&#8217;s probably a bit more analysis that could be done in terms of per-capita spending and household or individual income. Since &#8216;spending&#8217; is fully burdened, including all costs and not limited to only direct labor (salary) and fringe (benefits), such an analysis would need to include a reasonable assumption to factor out other indirect costs in order to arrive at a meaningful comparison. That factor would certainly have a geographic component since many overhead costs vary widely by location.</p>

<p>I found this report interesting because it starts to quantify the impact of contracting revenue on jurisdictions. This can be a bit nebulous in comparison to assessing the impact of payroll for government employees but is an important part of the overall picture. Understanding this picture can help jurisdictions that are currently heaviliy reliant on federal spending begin to plan how diversify themselves in the face of downward pressure on the federal budget.</p>

<hr />

<p>For those that are curious, the procurement data was manually captured from the GovWin report and joined to the county data using <a href="http://postgis.net">PostGIS</a>. It was then exported as a static <a href="http://geojson.org">GeoJSON</a> file for mapping using <a href="http://leafletjs.com">Leaflet</a>. Income and population data came from the US Census Bureau&#8217;s <a href="http://www.census.gov/acs/www/">American Community Survey</a> for 2012. The charts are built using <a href="http://d3js.org">D3</a>. All data, spatial and otherwise, is served as static content without the use of a map server of any kind.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Personal Geospatial Workflows, May 2014 Edition]]></title>
    <link href="http://blog.geomusings.com/2014/05/20/personal-geospatial-workflows/"/>
    <updated>2014-05-20T08:21:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/05/20/personal-geospatial-workflows</id>
    <content type="html"><![CDATA[<p>I have been spending the past few weeks dealing more with data and mapping than I have in quite a while. It&#8217;s given me a chance to regain my footing with map-making, reconnect with some end-user tools like <a href="http://www.arc2earth.com">Arc2Earth</a>, and build a little more proficiency with things like <a href="http://www.gdal.org/">GDAL</a>, <a href="http://qgis.org">QGIS</a>, and <a href="https://www.mapbox.com/tilemill/">TileMill</a>. Of course, I&#8217;ve been able to sneak in some coding as I&#8217;ve identified gaps in my workflow.</p>

<p>In a nutshell, I am building base maps for use on disconnected mobile devices. There are two styles of base maps; imagery (really more of an imagery/vector hybrid) and a high-contrast map for use on the outdoor devices and sourced only from vector data. In both cases, I am building MBTiles databases to support the requirement for disconnected operations and to provide consistency in data size and performance.</p>

<p>For the imagery base maps, I was faced with following a data request process that may or may not have resulted in getting imagery in a timely fashion. Alternatively, I was presented with the option of using a tiled map service to get the imagery. Given that I was just making basemaps, this would have been acceptable but for the spotty speed and reliability of the network connection. The ideal solution would be to get only the tiles I need, store them locally, create a geo-referenced image from them, and build a virtual raster table (VRT) for each level.</p>

<!--more-->


<p>Downloading the tiles was easy, but for the VRT to work, each tile needed to be geo-referenced. It was fairly easy to modify the venerable <a href="http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/globalmaptiles.py">globalmaptiles.py</a> to include a routine to create world file parameters for a specified tile. With this, I was able to write out an affine transformation world file for each tile I downloaded. I rolled this whole process up into a <a href="https://github.com/geobabbler/tile-grab">Python script that&#8217;s available here</a>. Please note that my goal was to create a VRT, so the script flattens out the tiling scheme so that all images are under the appropriate &#8220;Z&#8221; directory. (This particular server was an ArcGIS Server but the script doesn&#8217;t care as long as you can provide a valide URL template.)</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bat'><span class='line'>python tile_grab.py -b <span class="m">-158</span>;<span class="m">21</span>;<span class="m">-157</span>;<span class="m">22</span> -d E:\tiles\oahu_img\a -i false -z <span class="m">6</span> -u http:<span class="n">//www.someserver.net/arcgis/rest/services/ImagerySvc/MapServer/tile/{z}/{y}/{x}.png</span>
</span></code></pre></td></tr></table></div></figure>


<p>With the tiles downloaded and geo-referenced, it was easy to use the gdalbuildvrt utility to generate the VRT, which can be used in QGIS, TileMill, and ArcGIS Desktop as you prefer.</p>

<figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bat'><span class='line'>gdalbuildvrt <span class="m">6</span>.vrt <span class="m">6</span><span class="n">/*.png</span>
</span></code></pre></td></tr></table></div></figure>


<p>It seems a little odd to use tiles to make tiles, but I needed to add some additional data and styling to make the maps do what I needed to do. The downloaded tiles were just a stand-in for what would have been a standard raster data source. The range of useful resolution for a set of tiles is pretty narrow so you&#8217;ll probably need to grab a few levels and, even then, you&#8217;ll need to be careful how you use them. In most cases, using local raster/imagery is better but using tiles was fine for my use case and helped mitigate a byzantine data acquisition process. Here&#8217;s how I set the ranges in ArcGIS (left) and TileMill (right):</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/vrt_zoom2.png" /></p>

<p>Using this approach, I was able to successfully generate my maps using either of two technology mixes. I succeeded in using both TileMill and ArcGIS/Arc2Earth to generate my maps. I ended up doing most of the work in Arc2Earth due to the availability of command-line tools that helped me optimize performance.</p>

<p>Before attempting this method, it&#8217;s important to make sure that you are not violating any terms of service, license agreements, or attribution requirements in doing so. I knew this wasn&#8217;t an issue in my case, but such questions need to be answered before you start grabbing data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Virtual Rasters to Generate Contours in QGIS]]></title>
    <link href="http://blog.geomusings.com/2014/04/17/using-virtual-rasters-to-generate-contours-in-qgis/"/>
    <updated>2014-04-17T13:39:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/04/17/using-virtual-rasters-to-generate-contours-in-qgis</id>
    <content type="html"><![CDATA[<p>Every now and again, I am asked to make maps. It&#8217;s not my strongest suit, but it sometimes comes with the territory. My latest task, as mentioned in my previous post, involves building support for <a href="https://www.mapbox.com/developers/mbtiles/">MBTiles</a> databases into a mobile situational awareness tool. This is done so that the devices can have a persistent local basemap in the field. The need arose to ensure that the basemaps were high contrast to assist with visibility in bright sunlight. Something like this:</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour0.png" /></p>

<p>One of the requirements was to have topographic-map-like contours to indicate changes in elevation. Existing map services didn&#8217;t provide what we needed so it was necessary to build a custom map, which meant generating contour lines. It had been years since I had last done that with Esri tools, but I didn&#8217;t have any extension licenses available, so I turned to <a href="http://www.qgis.org/en/site/">QGIS</a> to get the job done this time.</p>

<!--more-->


<p>My area of interest was a portion of Virginia. Since I couldn&#8217;t find any pre-generated contours for the state, I turned to elevation models. There are numerous places to get such data, but I <a href="http://geoserve.asp.radford.edu/dems/va_dems.htm">downloaded some DEMs from Radford University</a> since they are already carved up by county. They are perhaps a bit dated, but they sufficed for this particular testing round.</p>

<p>It was easy to find <a href="http://boringnerdystuff.wordpress.com/2012/07/14/302/">guidance on how to generate contours in QGIS</a>. So I ran the process on a couple of adjacent counties and noticed that the edges didn&#8217;t line up, which was not surprising. My first thought was that I would need to merge the DEMs but, luckily, I stumbled across the virtual raster tool in QGIS. This tool provides a nice UI for building a <a href="http://www.gdal.org/gdal_vrttut.html">GDAL virtual raster</a> from a series of separate rasters specified by the user. This can be a bit cumbersome to do manually and this GUI tool was a real time saver. It can be found in QGIS Dufour here:</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour1.png" /></p>

<p>To make my life easier, I moved all of my DEMs into one folder so I could just point the tool at the folder. I filled in the name of the output file and took the defaults for everything else.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour2.png" /></p>

<p>Notice that the dialog shows me the GDAL command that I am building with the UI. Advanced users can even edit it here. This is a really nice feature that can help you get comfortable with GDAL. I am not a GDAL expert, nor am I particularly adept with raster operations so I found this to be a huge help and I plan to use it more.</p>

<p>The tool doesn&#8217;t change any data; it merely writes a text file so it works very quickly. The resulting virtual raster was done in a few seconds.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour3.png" /></p>

<p>With the data now &#8220;merged,&#8221; I was able to continue with data generation. For my purposes, 10-meter contours were more than sufficient. I generated a shapefile, but any QGIS-supported format is valid as an output. It should be noted that the &#8220;Attribute name&#8221; choice is not checked by default. Check this if you want to attach the elevation value to each line. Also notice that QGIS is again giving us the relevant GDAL command as we build it. This is very powerful as it gives you the option to use QGIS to prototype GDAL operations and then script them outside of QGIS, if you desire.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour6.png" /></p>

<p>This process took a little longer, thanks to Fauquier County, but still finished in about 90 seconds. The resulting contours were contiguous across counties, so my needs were met.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/qgis_contour8.png" /></p>

<p>I&#8217;m now in the process of styling the map in <a href="https://www.mapbox.com/tilemill/">TileMill</a> so that I can generate the databases. It&#8217;s good to occasionally take off my developer hat and put on that of a user. I&#8217;ve known for quite a while that QGIS stands toe-to-toe with any other desktop GIS software but this work got me to use some tools that I rarely ever touch. I was impressed with not only the speed, but also how smoothly the work flowed. My pedestrian laptop didn&#8217;t engage in nearly the same level of huffing and puffing that it does when I have to use other software. That may be a hidden &#8220;win&#8221; in that users can extend the useful life of their hardware by using tools that don&#8217;t tax it as much while producing the same results.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data, Apps, and Maps]]></title>
    <link href="http://blog.geomusings.com/2014/04/01/data/"/>
    <updated>2014-04-01T15:14:00-04:00</updated>
    <id>http://blog.geomusings.com/2014/04/01/data</id>
    <content type="html"><![CDATA[<p>It&#8217;s been a quiet month-and-a-half here on the blog, mostly owing to an abundance of project tasks. I recently started a short-term project to help one of my Federal customers extend data source support for an application they have been developing. This customer is technically a new one but the project team is made up of government developers that I have worked with on a few other projects so there is a great deal of familiarity.</p>

<p>The application, which has been under development for some time, is written in .Net and make use of the open-source (MIT) <a href="http://greatmaps.codeplex.com">GMap.NET</a> mapping library. The application features a desktop version running in Windows and a mobile version running on Android tablets. The .Net back end works seamlessly on both through the use of <a href="http://xamarin.com">Xamarin</a>, although I have not had the chance to get my hands dirty with that yet due to limits on Xamarin licenses and available Android devices. To its credit, GMap.NET seems to work fairly well in both environments.</p>

<!--more-->


<p>The project needed the ability to plug in custom base maps that would be accessible on the mobile devices which would not have internet connectivity, so I chose to use <a href="https://www.mapbox.com/developers/mbtiles/">MBTiles</a> as the provider format, given that it is widely supported and well-documented. This was my first time using the GMap.NET library and it was fairly easy to develop a new provider for it. <a href="https://github.com/geobabbler/MBTilesMapProvider">I have posted my provider code here</a>, though it may not work as a separate library due to some design choices in the core library.</p>

<p>From there, I actually had to move on to making some data sets for the various upcoming application test runs. This enabled me to reconnect with some old friends: <a href="https://www.mapbox.com/tilemill/">TileMill</a>, <a href="http://www.arc2earth.com">Arc2Earth</a>, and <a href="http://market.weogeo.com">WeoGeo</a>. I used WeoGeo for the bulk of my data acquisition, sticking to open data sources such as OSM and TIGER. The areas that I needed to work with were fairly small and WeoGeo&#8217;s feature of allowing me to upload KML to clip my data is really quite nice. If you still haven&#8217;t checked out WeoGeo for data acquisition, you really should. The customization tools even make for-purchase data sets fairly affordable.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/weoorder.png" /></p>

<p>I did my initial prototyping in TileMill to generate data sets to test the map provider. Once I moved on to building the actual data sets, I moved over to Arc2Earth. The main driver for that decision was that, in addtion to data acquired from WeoGeo, I had some data in a few government formats to integrate as well. Through GDAL and OGR, I could have accomplished that in TileMill, but I was able cut out a lot of data manipulation with Arc2Earth. That is why we keep multiple tools on the workbench.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/a2eexport2.png" /></p>

<p>To help fusing multiple databases, I had to take the step of developing <a href="https://github.com/geobabbler/MBTilesMerge">a GUI tool to merge MBTiles</a> databases. <a href="https://github.com/mapbox/mbutil/blob/master/patch">MapBox has a perfectly fine utility</a> to do this but this particular shop has seen fit to block console access on our Windows machines so I had to create my own tool to accomplish this task.</p>

<p>Every once in a while, it&#8217;s good to get back in touch with data and mapping workflows. It keeps me honest as a developer, even though mapping is not my strong suit. I have found working with GMap.NET interesting but I&#8217;m not sure I&#8217;d choose to do so again. There are a few design choices that fall into the &#8220;not how I would have done it&#8221; category but, primarily, I haven&#8217;t found any compelling reason to choose it over <a href="https://sharpmap.codeplex.com">SharpMap</a>, <a href="https://dotspatial.codeplex.com">DotSpatial</a>, or <a href="https://brutile.codeplex.com">BruTile</a>, which are open-source .Net libraries with which I am much more familiar.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Little Deeper with Node and PostGIS]]></title>
    <link href="http://blog.geomusings.com/2014/02/18/a-little-deeper-with-node-and-postgis/"/>
    <updated>2014-02-18T13:44:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/18/a-little-deeper-with-node-and-postgis</id>
    <content type="html"><![CDATA[<p>What does one do when presented with more snow days than expected? My friends in Colorado would probably do something outrageous like skiing, but I found it to be a great opportunity to catch up on some of my recreational coding. Specifically, I wanted to revisit the <a href="http://blog.geomusings.com/2013/12/11/building-a-simple-geodata-service-with-node-and-amazon-rds/">Node/PostGIS work I blogged about earlier</a>.</p>

<p>As fun as that project was, it was fairly limited in scope and I wanted to spread my wings a little more with Node. So I decided to build a more general-purpose wrapper around <a href="http://postgis.net">PostGIS</a>. Lately, I&#8217;ve become a bit obsessed with the idea that PostGIS may be the only GIS tool you really need in terms of data storage, management, and analytics. That&#8217;s probably a topic for another post but exploring that concept was a perfect premise for my continued explorations with Node.</p>

<p>I have been circling back to Node over the past few months to continue building my comfort level with it. I tend to eschew frameworks when i have learning something new because I want to get comfortable with the core before I start layering on abstraction. That was my approach with <a href="http://blog.geomusings.com/2013/04/25/simple-tile-viewer/">the tile viewer tool I built a while back</a>. For the recent post centered on Amazon RDS, I added Express into the mix, which has been a big help.</p>

<p>This time around, I wanted to dig a little deeper with the <a href="https://github.com/brianc/node-postgres">node-postgres</a> module and also make the application more modular. I wanted to build around a few core principles:</p>

<ol>
<li>Keep it RESTful (or as close to it as I could)</li>
<li>GeoJSON in/GeoJSON out (so&#8230;.vector only for now)</li>
<li>Let PostGIS do the heavy lifting</li>
</ol>


<!--more-->


<p><strong>Getting Started</strong></p>

<p>This time around, I elected to use my local PostgreSQL/PostGIS instance rather than Amazon RDS. This was mainly so I could keep my development isolated on one machine. I already had the basic infrastructure in place from my last time around, so I was able to quickly dive into the meat of things. I decided to scope my initial effort at the following:</p>

<ol>
<li>Return the contents of an entire table as GeoJSON, with the ability to choose between features (geometries and attributes) in a GeoJSON feature collection or just geometries in a GeoJSON geometry collection. This should support any table in the database.</li>
<li>Return those records in a table that intersect a geometry passed in as a parameter. The input geometry would be in GeoJSON format.</li>
<li>Return a JSON representation of a table&#8217;s schema.</li>
<li>Return a list of tables from the server. This is necessary in order to support the ability to query any available table.</li>
<li>Implement some simple, but application-specific logic to demonstrate extensibility.</li>
</ol>


<p>With these goals in mind, I decided to first tackle the issue of extensibility. I wanted to make it as painless as possible and <a href="http://timstermatic.github.io/blog/2013/08/17/a-simple-mvc-framework-with-node-and-express/">the strategy described in this post</a> seemed to fit the bill. I just had to add the following code block to my server.js (straight from the post):</p>

<figure class='code'><figcaption><span>snippet1.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="c1">// dynamically include routes (Controller)</span>
</span><span class='line'><span class="nx">fs</span><span class="p">.</span><span class="nx">readdirSync</span><span class="p">(</span><span class="s1">&#39;./controllers&#39;</span><span class="p">).</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span> <span class="p">(</span><span class="nx">file</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">if</span><span class="p">(</span><span class="nx">file</span><span class="p">.</span><span class="nx">substr</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;.js&#39;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">route</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;./controllers/&#39;</span> <span class="o">+</span> <span class="nx">file</span><span class="p">);</span>
</span><span class='line'>      <span class="nx">route</span><span class="p">.</span><span class="nx">controller</span><span class="p">(</span><span class="nx">app</span><span class="p">);</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>This will load any .js file in the controllers directory into the application. If they are written to the pattern expected by Express, new resource paths are exposed to the application. The post above describes a simple MVC implementation. Astute readers will note that my take is all &#8220;C&#8221; without &#8220;M&#8221; or &#8220;V.&#8221; I plan to refactor that later but it it was easier for me to keep track of things on this pass with code in one place.</p>

<p><strong>Getting Data</strong></p>

<p>With modularity out of the way, it was time work on the basic structure for getting data from the database. In <a href="https://github.com/geobabbler/node-gis-server/blob/master/controllers/core.js">core.js</a>, I defined a route with a URL template like &#8216;/vector/:schema/:table/:geom&#8217;. This would translate into something like http://localhost/vector/public/parcels/features, which would fetch a GeoJSON feature collection containing the contents of the parcels table. To do that, I need to know the name of the spatial column in the table, which the following helps me retrieve:</p>

<figure class='code'><figcaption><span>snippet2.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="kd">var</span> <span class="nx">meta</span> <span class="o">=</span> <span class="nx">client</span><span class="p">.</span><span class="nx">query</span><span class="p">(</span><span class="s2">&quot;select * from geometry_columns where f_table_name = &#39;&quot;</span> <span class="o">+</span> <span class="nx">tablename</span> <span class="o">+</span> <span class="s2">&quot;&#39; and f_table_schema = &#39;&quot;</span> <span class="o">+</span> <span class="nx">schemaname</span> <span class="o">+</span> <span class="s2">&quot;&#39;;&quot;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next code block shows how I capture the name of the spatial column and structure the main query, depending on the choice of features or geometry:</p>

<figure class='code'><figcaption><span>snippet3.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="nx">meta</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="s1">&#39;row&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">row</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">query</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">coll</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">spatialcol</span> <span class="o">=</span> <span class="nx">row</span><span class="p">.</span><span class="nx">f_geometry_column</span><span class="p">;</span>
</span><span class='line'>  <span class="k">if</span> <span class="p">(</span><span class="nx">geom</span> <span class="o">==</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">query</span> <span class="o">=</span> <span class="nx">client</span><span class="p">.</span><span class="nx">query</span><span class="p">(</span><span class="s2">&quot;select st_asgeojson(st_transform(&quot;</span> <span class="o">+</span> <span class="nx">spatialcol</span> <span class="o">+</span> <span class="s2">&quot;,4326)) as geojson, * from &quot;</span> <span class="o">+</span> <span class="nx">fullname</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span><span class="p">);</span>
</span><span class='line'>      <span class="nx">coll</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">type</span> <span class="o">:</span> <span class="s2">&quot;FeatureCollection&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">features</span> <span class="o">:</span> <span class="p">[]</span>
</span><span class='line'>      <span class="p">};</span>
</span><span class='line'>  <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">geom</span> <span class="o">==</span> <span class="s2">&quot;geometry&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">query</span> <span class="o">=</span> <span class="nx">client</span><span class="p">.</span><span class="nx">query</span><span class="p">(</span><span class="s2">&quot;select st_asgeojson(st_transform(&quot;</span> <span class="o">+</span> <span class="nx">spatialcol</span> <span class="o">+</span> <span class="s2">&quot;,4326)) as geojson from &quot;</span> <span class="o">+</span> <span class="nx">fullname</span> <span class="o">+</span> <span class="s2">&quot;;&quot;</span><span class="p">);</span>
</span><span class='line'>      <span class="nx">coll</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">type</span> <span class="o">:</span> <span class="s2">&quot;GeometryCollection&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">geometries</span> <span class="o">:</span> <span class="p">[]</span>
</span><span class='line'>      <span class="p">};</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="c1">//&#39;meta&#39; code block continues</span>
</span></code></pre></td></tr></table></div></figure>


<p>As can be seen above, the query will transform the output geometry to WGS84 and convert it to GeoJSON for me. So I&#8217;m sticking my third principle by leaning on PostGIS functions here. I plan to stick to GeoJSON&#8217;s default spatial reference of WGS84 for now. To roll up the query results into the appropriate GeoJSON object and return it, I handled the &#8216;row&#8217; and &#8216;end&#8217; events.</p>

<figure class='code'><figcaption><span>snippet4.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="c1">//roll up the results</span>
</span><span class='line'><span class="nx">query</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="s1">&#39;row&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="k">return</span> <span class="nx">res</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="s1">&#39;No data found&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span><span class='line'>      <span class="k">if</span> <span class="p">(</span><span class="nx">geom</span> <span class="o">==</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">coll</span><span class="p">.</span><span class="nx">features</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">geojson</span><span class="p">.</span><span class="nx">getFeatureResult</span><span class="p">(</span><span class="nx">result</span><span class="p">,</span> <span class="nx">spatialcol</span><span class="p">));</span> <span class="c1">//use helper function</span>
</span><span class='line'>      <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="nx">geom</span> <span class="o">==</span> <span class="s2">&quot;geometry&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>          <span class="kd">var</span> <span class="nx">shape</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">result</span><span class="p">.</span><span class="nx">geojson</span><span class="p">);</span>
</span><span class='line'>          <span class="nx">coll</span><span class="p">.</span><span class="nx">geometries</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="nx">shape</span><span class="p">);</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">});</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//send the results</span>
</span><span class='line'><span class="nx">query</span><span class="p">.</span><span class="nx">on</span><span class="p">(</span><span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="nx">res</span><span class="p">.</span><span class="nx">setHeader</span><span class="p">(</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">,</span> <span class="s1">&#39;application/json&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">res</span><span class="p">.</span><span class="nx">send</span><span class="p">(</span><span class="nx">coll</span><span class="p">);</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>I wrote a helper function to roll up GeoJSON features:</p>

<figure class='code'><figcaption><span>snippet5.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="nx">exports</span><span class="p">.</span><span class="nx">getFeatureResult</span> <span class="o">=</span> <span class="kd">function</span><span class="p">(</span><span class="nx">result</span><span class="p">,</span> <span class="nx">spatialcol</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">props</span> <span class="o">=</span> <span class="k">new</span> <span class="nb">Object</span><span class="p">;</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">crsobj</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>          <span class="s2">&quot;type&quot;</span> <span class="o">:</span> <span class="s2">&quot;name&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="s2">&quot;properties&quot;</span> <span class="o">:</span> <span class="p">{</span>
</span><span class='line'>              <span class="s2">&quot;name&quot;</span> <span class="o">:</span> <span class="s2">&quot;urn:ogc:def:crs:EPSG:6.3:4326&quot;</span>
</span><span class='line'>          <span class="p">}</span>
</span><span class='line'>      <span class="p">};</span>
</span><span class='line'>      <span class="c1">//builds feature properties from database columns</span>
</span><span class='line'>      <span class="k">for</span> <span class="p">(</span><span class="kd">var</span> <span class="nx">k</span> <span class="k">in</span> <span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>          <span class="k">if</span> <span class="p">(</span><span class="nx">result</span><span class="p">.</span><span class="nx">hasOwnProperty</span><span class="p">(</span><span class="nx">k</span><span class="p">))</span> <span class="p">{</span>
</span><span class='line'>              <span class="kd">var</span> <span class="nx">nm</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="o">+</span> <span class="nx">k</span><span class="p">;</span>
</span><span class='line'>              <span class="k">if</span> <span class="p">((</span><span class="nx">nm</span> <span class="o">!=</span> <span class="s2">&quot;geojson&quot;</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="nx">nm</span> <span class="o">!=</span> <span class="nx">spatialcol</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>                  <span class="nx">props</span><span class="p">[</span><span class="nx">nm</span><span class="p">]</span> <span class="o">=</span> <span class="nx">result</span><span class="p">[</span><span class="nx">k</span><span class="p">];</span>
</span><span class='line'>              <span class="p">}</span>
</span><span class='line'>          <span class="p">}</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>      <span class="k">return</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">type</span> <span class="o">:</span> <span class="s2">&quot;Feature&quot;</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">crs</span> <span class="o">:</span> <span class="nx">crsobj</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">geometry</span> <span class="o">:</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">parse</span><span class="p">(</span><span class="nx">result</span><span class="p">.</span><span class="nx">geojson</span><span class="p">),</span>
</span><span class='line'>          <span class="nx">properties</span> <span class="o">:</span> <span class="nx">props</span>
</span><span class='line'>      <span class="p">};</span>
</span><span class='line'>  <span class="p">};</span>
</span></code></pre></td></tr></table></div></figure>


<p>So that&#8217;s basic data retrieval. How about that spatial intersect?</p>

<p><strong>A Simple Spatial Query</strong></p>

<p>One thing I failed to mention in the above section, is that all of that is exposed through an HTTP GET request. For this next function, I&#8217;m going to use a POST. I went back and forth on that but came down on the side of POST due to the potential for a user to send a very verbose input shape. The function is designed to accept JSON as the body of the request, which would be done in curl like this:</p>

<figure class='code'><figcaption><span>snippet6.bat </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bat'><span class='line'>curl -X POST -d <span class="s2">&quot;{ \&quot;</span>type\<span class="s2">&quot;: \&quot;</span>Point\<span class="s2">&quot;, \&quot;</span>coordinates\<span class="s2">&quot;: [-98.35, 39.7] }&quot;</span> -H <span class="s2">&quot;Content-Type: application/json&quot;</span> http:<span class="n">//localhost:3000/vector/public/states_gen/features/intersect</span>
</span></code></pre></td></tr></table></div></figure>


<p>The above action returns the state of Kansas (I knew you were wondering). To make this happen, there are only three things that are different. First, the URL is defined a POST and, second, the code needs to capture the input shape. The first few lines are:</p>

<figure class='code'><figcaption><span>snippet7.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'>   <span class="cm">/**</span>
</span><span class='line'><span class="cm">  * retrieve all features that intersect the input GeoJSON geometry</span>
</span><span class='line'><span class="cm">  */</span>
</span><span class='line'>  <span class="nx">app</span><span class="p">.</span><span class="nx">post</span><span class="p">(</span><span class="s1">&#39;/vector/:schema/:table/:geom/intersect&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">,</span> <span class="nx">next</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="c1">//console.log(JSON.stringify(req.body));</span>
</span><span class='line'>      <span class="kd">var</span> <span class="nx">queryshape</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">);</span>
</span><span class='line'>  <span class="c1">//continue with the rest of app.post</span>
</span></code></pre></td></tr></table></div></figure>


<p>I stringify the JSON since I have to insert it into my SQL. This brings me to the third difference here, the query. This time, I am using ST_INTERSECTS to filter down the response. So, depending on the choice of features or geometry, the query will be similar to:</p>

<p><strong>&#8220;select st_asgeojson(st_transform(&#8221; + spatialcol + &#8220;,4326)) as geojson, * from &#8221; + fullname + &#8221; where ST_INTERSECTS(&#8221; + spatialcol + &#8220;, ST_SetSRID(ST_GeomFromGeoJSON(&#8216;&#8221; + queryshape + &#8220;&#8217;),4326));&#8221;</strong></p>

<p>The rest of the process is similar to the basic query above. With a well-exercised data access pattern in place, querying table schema and layer lists become trivial. Since GeoJSON doesn&#8217;t cover these topics, I had to roll my own. I won&#8217;t detail the output but the queries are below.</p>

<figure class='code'><figcaption><span>snippet8.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="c1">//SQL to retrieve schema</span>
</span><span class='line'><span class="c1">//var sql = &quot;SELECT n.nspname as schemaname,c.relname as table_name,a.attname as column_name,format_type(a.atttypid, a.atttypmod) AS //type,col_description(a.attrelid, a.attnum) as comments&quot;;</span>
</span><span class='line'><span class="c1">//sql = sql + &quot; FROM pg_class c INNER JOIN pg_namespace n ON c.relnamespace = n.oid LEFT JOIN pg_attribute a ON a.attrelid = c.oid&quot;;</span>
</span><span class='line'><span class="c1">//sql = sql + &quot; WHERE a.attnum &gt; 0 and c.relname = &#39;&quot; + tablename + &quot;&#39; and n.nspname = &#39;&quot; + schemaname + &quot;&#39;;&quot;;</span>
</span><span class='line'>
</span><span class='line'><span class="c1">//SQL to retrieve layer list</span>
</span><span class='line'><span class="c1">//sql = &quot;SELECT &#39;geometry&#39; AS geotype, * FROM geometry_columns UNION SELECT &#39;geography&#39; as geotype, * FROM geography_columns;&quot;;</span>
</span></code></pre></td></tr></table></div></figure>


<p>So this gives me everything I need for an all-purpose interface into PostGIS from Node. I could spend the rest of the year similarly wrapping the hundreds of spatial functions in PostGIS but the real power of extensibility is the ability to tailor an application for one&#8217;s own needs, based upon one&#8217;s detailed understanding of their own data and logic.</p>

<p><strong>Adding Some Customization</strong></p>

<p>To do this, I fell back to the data for <a href="http://leonardtown.somd.com">Leonardtown, Maryland</a> that have used in a <a href="http://blog.geomusings.com/2013/06/18/geojson-on-github-now-what/">couple</a> of previous <a href="http://blog.geomusings.com/2011/10/13/cartodb-leaflet-easy/">posts</a>. I am simply going to expose the ability to query residential or commercial buildings from the data set. For this, all of the prep work is done at the top of the function by simply preparing a WHERE clause.</p>

<figure class='code'><figcaption><span>snippet9.js </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="nx">app</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/leonardtown/buildings/:geom&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">,</span> <span class="nx">next</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">client</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">pg</span><span class="p">.</span><span class="nx">Client</span><span class="p">(</span><span class="nx">app</span><span class="p">.</span><span class="nx">conString</span><span class="p">);</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">geom</span> <span class="o">=</span> <span class="nx">req</span><span class="p">.</span><span class="nx">params</span><span class="p">.</span><span class="nx">geom</span><span class="p">.</span><span class="nx">toLowerCase</span><span class="p">();</span>
</span><span class='line'>  <span class="k">if</span> <span class="p">((</span><span class="nx">geom</span> <span class="o">!=</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="nx">geom</span> <span class="o">!=</span> <span class="s2">&quot;geometry&quot;</span><span class="p">))</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">(</span><span class="s2">&quot;Resource &#39;&quot;</span> <span class="o">+</span> <span class="nx">geom</span> <span class="o">+</span> <span class="s2">&quot;&#39; not found&quot;</span><span class="p">);</span>
</span><span class='line'>      <span class="k">return</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">tablename</span> <span class="o">=</span> <span class="s2">&quot;leonardtown_bldgs&quot;</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">schemaname</span> <span class="o">=</span> <span class="s2">&quot;public&quot;</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">fullname</span> <span class="o">=</span> <span class="nx">schemaname</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nx">tablename</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">spatialcol</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">proptype</span> <span class="o">=</span> <span class="nx">req</span><span class="p">.</span><span class="nx">query</span><span class="p">.</span><span class="nx">type</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">whereclause</span> <span class="o">=</span> <span class="s2">&quot;;&quot;</span><span class="p">;</span>
</span><span class='line'>  <span class="k">if</span> <span class="p">(</span><span class="k">typeof</span> <span class="nx">proptype</span> <span class="o">!=</span> <span class="s2">&quot;undefined&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="k">if</span> <span class="p">(</span><span class="nx">proptype</span><span class="p">.</span><span class="nx">toLowerCase</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;all&quot;</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>          <span class="nx">whereclause</span> <span class="o">=</span> <span class="s2">&quot; where structure_ = &#39;&quot;</span> <span class="o">+</span> <span class="nx">proptype</span> <span class="o">+</span> <span class="s2">&quot;&#39;;&quot;</span><span class="p">;</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">coll</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">sql</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">;</span>
</span><span class='line'>  <span class="c1">//logic continues from here</span>
</span></code></pre></td></tr></table></div></figure>


<p>The primary difference here are that I am using a GET with a query string since I&#8217;m not concerned with data size and that I&#8217;m building a WHERE clause on a specific column name. What&#8217;s not shown is that, farther down, I don&#8217;t need to query the name of the spatial column so I can cut out that step. I can do this because I understand my own data so I can be more succinct that if I were writing a more generic function. Using this approach I can also write more complex custom logic in my database, call it from Node, and send the response. In other words, standard web application behavior.</p>

<p>In order to expose this application-specific logic, I just needed expose it in a separate leonardtown.js file and drop it into the &#8216;controllers&#8217; directory.</p>

<p><strong>Wrapping Up</strong></p>

<p>This post was bit longer than usual but there was lot of ground to cover. I feel like I&#8217;m getting more comfortable with the Node ecosystem though I&#8217;m still a bit wobbly. My next step is probably to dive a little deeper into the MVC side of things with something like <a href="http://sailsjs.org">Sails</a>. Having a familiar face like PostGIS on the back end is helping me as I figure out how to perform more meaningful tasks with Node and its related tools.</p>

<p>If you want to check out the full code for this application, it is here: <a href="https://github.com/geobabbler/node-gis-server">https://github.com/geobabbler/node-gis-server</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[URL Utility for Twitter Direct Messages]]></title>
    <link href="http://blog.geomusings.com/2014/02/17/url-utility-for-twitter-direct-messages/"/>
    <updated>2014-02-17T12:40:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/17/url-utility-for-twitter-direct-messages</id>
    <content type="html"><![CDATA[<p>When Twitter opened up direct messages (DMs) so that anyone could DM anyone else, they made it much more difficult to DM links. This is understandable as it makes DM spamming much more difficult. I have found that links to tweets and links to GitHub seem to work but many, many others do not.</p>

<p>This has led to a variety of strategies for sending a link, including cumbersome entry as something like &#8220;www[dot]microsoft[dot]com&#8221; or similar. The strategy I settled on was to replace slashes with pipes and dots with asterisks(http:||www*microsoft*com). It&#8217;s easy and relatively risk free in terms of <a href="http://tools.ietf.org/html/rfc3986">RFC 3986</a>, but I&#8217;m still not a huge fan of typing and, if there will be errors introduced, it will most likely occur between the chair and the keyboard. As a result, I wrote myself a dirt-simple little tool to make my life easier. In the spirit of <a href="http://geohipster.com/2014/02/11/lyzi-diamond-make-things-put-internet/">making things and putting them on the internet</a>, I posted it as a utility on this site and <a href="http://blog.geomusings.com/samples/urltoken.html">it can be found here</a>. It seems to do just enough to satisfy the Twitter filter, which is really all I&#8217;m after.</p>

<p>It&#8217;s very simple but it makes my life easier. If I send you a link in this format, you can unroll it here. If you want to send a link, you can roll it up here. Feel free to try it and I hope you find it useful. Feedback is always welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Initial Thoughts On the DC DevSummit]]></title>
    <link href="http://blog.geomusings.com/2014/02/13/initial-thoughts-on-the-dc-devsummit/"/>
    <updated>2014-02-13T17:19:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/13/initial-thoughts-on-the-dc-devsummit</id>
    <content type="html"><![CDATA[<p>This week, I attended the first-ever <a href="http://www.esri.com">Esri</a> <a href="http://www.esri.com/events/devsummit-dc">DC DevSummit</a> which followed the <a href="http://www.esri.com/events/federal">Federal GIS Conference</a> (please switch it back to &#8220;FedUC&#8221;). This event, intended and a smaller, Federally-focused, companion to the annual Palm Springs DevSummit, came together quickly but was very well-attended with about 300 attendees.</p>

<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/devsummit.jpg" /></p>

<p>It was interesting to note that the most well-attended sessions of the day had to do with Javascript (every Javascript session had over 100 participants). As more and more organizations update their IT infrastructures, the acceleration away from plug-ins seems to be picking up pace. The most common refrain amongst attendees in that regard is that continued standardization on IE8 remains the biggest impediment to sunsetting things like Flex and Silverlight, but the logjam seems to be starting to break loose.</p>

<!--more-->


<p>Despite the photo I posted, <a href="http://twitter.com/ajturner">Andrew Turner&#8217;s</a> open-source session was fairly well-attended. It was good to see tools like <a href="https://github.com/Esri/koop">Koop</a> in action. It is clear that there is real effort going on within Esri to produce open-source tools and that the people working on them are genuinely committed to them. That said, it is obviously still early days and such efforts are clearly swimming upstream against the preponderance of corporate culture. This is an effort that will best be judged over the long haul.</p>

<p>I also have to give a shout out to <a href="http://twitter.com/agup">Andy Gup</a>, whom I finally met at this conference. By chance, I sat through three of his sessions. He is one of the best technical presenters I have ever seen and Esri should require junior staff to sit through his sessions to see how it should be done.</p>

<p>I found the DevSummit generally worthwhile and I was impressed with how well it came together given the short timeframe. Since I was encouraged to blog suggestions, here are a few:</p>

<ol>
<li><p>Make a DevSummit a permanent fixture as a follow-on to the Federal User Conference each year. The format of this year&#8217;s Federal conference can be tweaked but stay mainly focused on user-centric use cases and some intro-level discussions of technologies. A DevSummit would mark a shift in to much more technical content with advanced discussions of APIs, security, techniques and best practices.</p></li>
<li><p>Pull out the stops. As I indicated in item 1, ramp up the technical content, in comparison to that of the Federal conference, significantly. This year&#8217;s event started down that path. I would keep the technical content of the Federal conference on the bunny slopes and shift to the black diamonds for the DevSummit.</p></li>
<li><p>Expand to two days. It was a quick day and there were definitely sessions I would have liked to have gotten to. Additionally, it would be good to see some user content, maybe some lightning talks on the evening between the days. One of the biggest challenges working in the Federal space is the compartmentalization between and within agencies. With the DevSummit drawing interested people to one location, it would be good to get more opportunities to interact with other developers and exchange information.</p></li>
<li><p><a href="http://www.gsa.gov/portal/category/102371?utm_source=OCM&amp;utm_medium=print-radio&amp;utm_term=HP_13_SpecialTopics_fedramp&amp;utm_campaign=shortcuts">FedRAMP</a> and <a href="http://en.wikipedia.org/wiki/Federal_Information_Security_Management_Act_of_2002">FISMA</a> were non-existent this year. Jim Barry explained to me that these were areas where content simply wasn&#8217;t ready to go in the short time given to prepare for the DevSummit. That&#8217;s fair, and I certainly respect the decision to not provide content that could not be done well. These are, however, core issues for anyone doing development for the Federal Government. Next year&#8217;s DC DevSummit really needs to have content for developers attempting to deploy to FedRAMP and achieve FISMA compliance with Esri tools.</p></li>
</ol>


<p>Those my quick hits. I&#8217;ll probably have more after I have time to digest what I saw and dig out from the backlog resulting from two days out of the office. Kudos to <a href="http://twitter.com/jimbarry">Jim Barry</a> for his work pulling the DC DevSummit together.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Meanwhile, Over at Zekiah...]]></title>
    <link href="http://blog.geomusings.com/2014/02/06/meanwhile/"/>
    <updated>2014-02-06T11:34:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/06/meanwhile</id>
    <content type="html"><![CDATA[<p>I don&#8217;t usually cross-pollinate between this, my personal blog, and the company blog over at <a href="http://www.zekiah.com">Zekiah</a>. One of the great things about working at a place like Zekiah, however, is the opportunity to work with smart people and see what they are doing. At times, my colleagues will share components of their work on the company blog. We encourage this, and the experimentation that leads to the posts, as a way to keep our technical capabilities fresh and to also showcase what we do in a way that goes beyond the typical capabilities statements that exist on every site. My colleagues have been pretty busy but have managed to take some time to write a few posts about their work:</p>

<ul>
<li><a href="http://www.zekiah.com/index.php?q=blog/2014/02/04/esri-cityengine-unity-40-and-oculus-rift">Esri CityEngine, Unity 4.0 and the Oculus Rift</a> - My colleague, <a href="http://twitter.com/DanEntzian">Dan Entzian</a>, is an avid gamer, a great developer, and a smart GIS guy. This post combines those interests by showing how to bring cities created in Esri&#8217;s <a href="http://www.esri.com/software/cityengine/">CityEngine</a> into gaming environments like <a href="http://unity3d.com/">Unity</a> and integrate them with the <a href="http://www.oculusvr.com/">Oculus Rift</a> virtual reality display. It&#8217;s a quick, but detailed, read that shows the interactions possible between geospatial tools and games.</li>
</ul>


<!--more-->


<p style="text-align:center;"> <img src="http://blog.geomusings.com/images/posts/unity_ide.png" /></p>

<ul>
<li><p><a href="http://www.zekiah.com/index.php?q=blog/2014/01/23/using-awk-ease-your-csv-manipulation">Using AWK to ease your CSV manipulation</a> - <a href="http://twitter.com/hugoestr">Hugo Estrada</a> shows how to use an old, but still effective, tool, <a href="http://www.grymoire.com/Unix/Awk.html">AWK</a>, to process GPS data for use in GIS software. This post is a great reminder that the best tool for the job may already be sitting there at our command prompt waiting for us.</p></li>
<li><p><a href="http://www.zekiah.com/index.php?q=blog/2013/12/18/exporting-esri-silverlight-graphic-layer-google-earth-part-2">Exporting ESRI Silverlight Graphic Layer to Google Earth, Part 2</a> - While Silverlight is, politely speaking, passe, we have a few customers that are still attached to it. Generally, the systems that are using it are accredited systems of record so a rip-and-replace of Silverlight (or any other component) is simply not feasible without a significant paperwork drill. So we try to help our customers keep those systems as useful for their end users as possible. This post, also by Dan Entzian, illustrates how we did that in one case. A follow up to <a href="http://www.zekiah.com/index.php?q=blog/2011/10/11/exporting-esri-silverlight-graphic-layer-google-earth">an older post</a>, this post was done in response to an e-mail inquiry from a reader of the previous post.</p></li>
<li><p><a href="http://www.zekiah.com/index.php?q=blog/2013/12/17/overview-clojure">An Overview of Clojure</a> - In this post, Hugo Estrada takes a look a <a href="http://clojure.org/">Clojure</a>, a variant of the Lisp programming language, and reports on his experience at Clojure Con. I found this particularly interesting since, as a lifelong programmer, I am always interested in new languages (even if it is getting harder to find the time to tinker with them myself).</p></li>
<li><p><a href="http://www.zekiah.com/index.php?q=blog/2014/01/02/generating-physical-schemas-pim">Generating Physical Schemas From a PIM</a> - Okay, this one was written by me, but the work is pretty interesting and involved the efforts of a few co-workers, including Barry Schimpf and Dan Entzian. This post describes a tool that we developed as part of our overall approach to geospatial data model management. This script generator produces SQL scripts for either <a href="http://postgis.net/">PostGIS</a> or <a href="http://www.gaia-gis.it/gaia-sins/">SpatiaLite</a> that enable a user to create spatial databases that are compliant with a data model. The information for the data model (which is always user-defined, not proscribed by us) is stored in what we call the platform independent model, or <a href="http://www.zekiah.com/index.php?q=blog/topics/pim">PIM</a>. We&#8217;ve used the PIM, its encapsulating API, and tools to good effect for a couple of customers now. This post attempts to provide a concrete picture of what can be an abstract concept.</p></li>
</ul>


<p>Since Zekiah is a services company, rather than a platform company, we get to work with a broad range of technologies in support of our customers, in addition to our own internal research. This makes each day pretty interesting and can also make for lively conversation at company events. As the posts above showcase, my colleagues are working on some interesting things and it&#8217;s a pleasure to work them each day.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Esri Federal GIS Conference Features Immersion Summits]]></title>
    <link href="http://blog.geomusings.com/2014/02/05/esri-federal-gis-conference-features-immersion-summits/"/>
    <updated>2014-02-05T20:19:00-05:00</updated>
    <id>http://blog.geomusings.com/2014/02/05/esri-federal-gis-conference-features-immersion-summits</id>
    <content type="html"><![CDATA[<p>The 2014 installment of the <a href="http://www.esri.com/events/federal">Esri Federal GIS Conference</a> (formerly known as the Federal User Conference) happens next week. I have attended the event off and on since its inception. While I originally was drawn by the presence of a large geo-related event in my local area, that gap has been filled by numerous, smaller events from various sources in the past few years. The FedUC has traditionally had something of an identity crisis, with the content often feeling a bit diffuse and somewhat rushed over its quick, two-day schedule.</p>

<p>To Esri&#8217;s credit, and probably in recognition of the changing economics regarding travel for Federal employees, they have continued to fine-tune the event when they could have easily walked away from it. The main problem with the FedUC is the fact that the Federal Government performs a wide array of functions that are difficult to cover well over two days in a conventional format. Esri has tackled that problem this year with the introduction of <a href="http://www.esri.com/events/federal/agenda/immersion-summits">&#8220;immersion summits,&#8221;</a> which are three-and-a-half hour sessions dedicated to specific domain areas.</p>

<!--more-->


<p>For example, check out the Natural Resources Immersion Summit led by my friend <a href="http://twitter.com/jsteffenson">John Steffenson</a>. The agenda features speakers from USDA, USGS, the US Forest Service, the EPA, the Fish and Wildlife Service, and Department of Interior, representing over 100 years of higher education dedicated to natural resources management. The session also has slots for several demos of systems deployed in natual resources domain.</p>

<p>The session&#8217;s content seems to be designed take a holistic view, including fiscal, logistical, technical, and demographic challenges among others. This type of deep focus is an intriguing change to event&#8217;s focus. While nautral resources is not my usual domain, I do like to drop into other sessions as I find that uses cases from other domains can give me ideas to bring back to my work. I think this new format could maximize the use of the compact schedule of the Federal GIS Conference and begin to give the event its own identity.</p>
]]></content>
  </entry>
  
</feed>
